import streamlit as st
import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from streamlit_calendar import calendar
import os
import shutil
import time
import json
import re
import uuid
import wave
import struct
import gzip
import hashlib
import zipfile
import base64
import tempfile
from io import BytesIO
from docx import Document
from docx.oxml.table import CT_Tbl
from docx.oxml.text.paragraph import CT_P
from docx.table import Table, _Cell
from docx.text.paragraph import Paragraph
from openai import OpenAI
import websocket
from audio_recorder_streamlit import audio_recorder
import plotly.graph_objects as go
import plotly.express as px
from collections import Counter
import numpy as np

try:
    import audioop  # removed in newer Python versions; optional in this app
except Exception:
    audioop = None

def _load_dotenv(dotenv_path: str = ".env") -> None:
    """Load simple KEY=VALUE pairs from .env into os.environ (no external deps)."""
    if not os.path.exists(dotenv_path):
        return
    try:
        with open(dotenv_path, "r", encoding="utf-8") as fh:
            for raw_line in fh:
                line = raw_line.strip()
                if not line or line.startswith("#"):
                    continue
                if "=" not in line:
                    continue
                key, value = line.split("=", 1)
                key = key.strip()
                value = value.strip().strip("'").strip('"')
                if not key:
                    continue
                os.environ.setdefault(key, value)
    except Exception:
        return


def _get_setting(name: str, default: str = "") -> str:
    """Read settings from Streamlit secrets first, then env vars."""
    try:
        if hasattr(st, "secrets") and name in st.secrets:
            return str(st.secrets[name])
    except Exception:
        pass
    return os.getenv(name, default)


_load_dotenv()

try:
    import pypandoc
except Exception:
    pypandoc = None

try:
    import win32com.client
    import pythoncom
except Exception:
    win32com = None
    pythoncom = None

HAS_PYPANDOC = pypandoc is not None
HAS_WIN32_COM = win32com is not None

# --- é…ç½®åŒº ---
LEGACY_UPLOAD_DIR = "uploads"
LEGACY_BACKUP_DIR = "backups"
LEGACY_DB_PATH = "my_lab_data.db"
DATA_DIR = "data"


def _get_streamlit_user_email() -> str | None:
    """Best-effort: return the email of the signed-in Streamlit user (Cloud/private apps)."""
    try:
        if hasattr(st, "user"):
            info = st.user.to_dict() or {}
            email = info.get("email")
            if email:
                return str(email).strip().lower()
    except Exception:
        pass
    return None


def get_storage_paths() -> dict:
    """
    Multi-user isolation:
    - If Streamlit provides a signed-in user email, store data in `data/users/<hash>/`.
    - Otherwise (local/dev), fall back to legacy paths in the repo root.
    """
    override_email = _get_setting("LAB_DIARY_USER_EMAIL", "").strip().lower()
    email = override_email or _get_streamlit_user_email()
    if email:
        digest = hashlib.sha256(email.encode("utf-8")).hexdigest()[:16]
        root = os.path.join(DATA_DIR, "users", digest)
        upload_dir = os.path.join(root, "uploads")
        backup_dir = os.path.join(root, "backups")
        db_path = os.path.join(root, "my_lab_data.db")
        user_label = email
    else:
        root = "."
        upload_dir = LEGACY_UPLOAD_DIR
        backup_dir = LEGACY_BACKUP_DIR
        db_path = LEGACY_DB_PATH
        user_label = "local"
    os.makedirs(upload_dir, exist_ok=True)
    os.makedirs(backup_dir, exist_ok=True)
    return {
        "user_label": user_label,
        "root": root,
        "upload_dir": upload_dir,
        "backup_dir": backup_dir,
        "db_path": db_path,
    }

# --- ç«å±±å¼•æ“è¯­éŸ³è¯†åˆ«é…ç½® ---
VOLC_ASR_WS_URL = _get_setting("VOLC_ASR_WS_URL", "wss://openspeech.bytedance.com/api/v3/sauc/bigmodel_async")
VOLC_ASR_APP_KEY = _get_setting("VOLC_ASR_APP_KEY", "")
VOLC_ASR_ACCESS_KEY = _get_setting("VOLC_ASR_ACCESS_KEY", "")
VOLC_ASR_RESOURCE_ID = _get_setting("VOLC_ASR_RESOURCE_ID", "volc.bigasr.sauc.duration")
VOLC_AUDIO_SAMPLE_RATE = 16000
VOLC_AUDIO_SAMPLE_WIDTH = 2
VOLC_AUDIO_CHANNELS = 1
VOLC_AUDIO_FORMAT = "pcm"
VOLC_AUDIO_CHUNK_MS = 200

# --- DeepSeek AI é…ç½® ---
DEEPSEEK_API_KEY = _get_setting("DEEPSEEK_API_KEY", "")
DEEPSEEK_BASE_URL = _get_setting("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
DEEPSEEK_MODEL = _get_setting("DEEPSEEK_MODEL", "deepseek-chat")

# --- è®¾è®¡é£æ ¼é…ç½® ---
# è‰²å½©ç³»ç»Ÿ
COLORS = {
    'primary': '#1e293b',      # æ·±è“ç° - ä¸»è‰²
    'secondary': '#334155',    # ä¸­è“ç° - è¾…åŠ©è‰²
    'background': '#f8fafc',   # ææµ…ç° - èƒŒæ™¯
    'accent': '#3b82f6',       # ç§‘æŠ€è“ - å¼ºè°ƒ
    'success': '#10b981',      # æˆåŠŸç»¿
    'warning': '#f59e0b',      # è­¦å‘Šæ©™
    'error': '#ef4444',        # é”™è¯¯çº¢
    'info': '#06b6d4',         # ä¿¡æ¯è“
    'research': '#8b5cf6',     # ç§‘ç ”ç´«
    'clinical': '#06b6d4',     # ä¸´åºŠé’
    'course': '#f59e0b',       # è¯¾ç¨‹é‡‘
    'other': '#6b7280',        # ä¸­æ€§ç°
}

# å­—ä½“
FONTS = {
    'family': 'Inter, -apple-system, BlinkMacSystemFont, sans-serif',
    'mono': 'JetBrains Mono, monospace'
}

for d in [LEGACY_UPLOAD_DIR, LEGACY_BACKUP_DIR]:
    os.makedirs(d, exist_ok=True)

# ==================== å·¥å…·å‡½æ•° ====================
def get_versioned_upload_path(filename):
    """Return upload path plus versioned filename to avoid overwriting."""
    upload_dir = get_storage_paths()["upload_dir"]
    base, ext = os.path.splitext(filename)
    candidate = filename
    idx = 1
    while os.path.exists(os.path.join(upload_dir, candidate)):
        candidate = f"{base}_v{idx}{ext}"
        idx += 1
    return os.path.join(upload_dir, candidate), candidate

def normalize_task_row(row):
    if isinstance(row, dict):
        return row
    if hasattr(row, "to_dict"):
        return row.to_dict()
    return row

def sanitize_filename(value, default="record"):
    value = re.sub(r"[^\w\-]+", "_", value).strip("_")
    return value or default

def shorten_task_name(name: str, max_length: int = 28) -> str:
    """å¼ºåˆ¶ä»»åŠ¡åç®€æ´ä¸”å»é™¤å†—ä½™æ ‡ç‚¹"""
    if not name:
        return "æœªå‘½åä»»åŠ¡"
    clean = re.sub(r"\s+", " ", str(name)).strip()
    clean = re.sub(r"[ï¼Œã€‚,.ï¼›;ã€ï¼š:]+$", "", clean)
    if len(clean) > max_length:
        clean = clean[:max_length].rstrip() + "â€¦"
    return clean

# ==================== AI åŠŸèƒ½ ====================
def get_ai_client():
    """è·å– DeepSeek AI å®¢æˆ·ç«¯"""
    if not DEEPSEEK_API_KEY:
        return None
    return OpenAI(base_url=DEEPSEEK_BASE_URL, api_key=DEEPSEEK_API_KEY)

def ai_polish_text(client, text, extra_instruction=None):
    """AI æ¶¦è‰²åŠŸèƒ½"""
    if not text:
        return "è¯·å…ˆè¾“å…¥æ–‡æœ¬ã€‚"
    user_content = text
    if extra_instruction:
        user_content = f"{text}\n\n[è¡¥å……è¦æ±‚]\n{extra_instruction}"
    try:
        response = client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç¥ç»ç§‘å­¦åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„å®éªŒè®°å½•æ¶¦è‰²ä¸ºå­¦æœ¯é£æ ¼ã€‚ç›´æ¥è¾“å‡ºç»“æœã€‚"},
                {"role": "user", "content": user_content}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {e}"

def ai_extract_metadata(client, text):
    """
    AIåªæå–å…ƒæ•°æ®ï¼Œä¸ä¿®æ”¹åŸå§‹å†…å®¹
    è¿”å›: {date, task_name, category, tags}
    """
    try:
        response = client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªå…ƒæ•°æ®æå–åŠ©æ‰‹ã€‚ä»å®éªŒè®°å½•ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œä½†ä¸è¦ä¿®æ”¹åŸå§‹è®°å½•å†…å®¹ï¼š
1. æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
2. ä»»åŠ¡/å®éªŒåç§°
3. ç±»åˆ«ï¼ˆç§‘ç ”/ä¸´åºŠ/è¯¾ç¨‹/å…¶ä»–ï¼‰
4. æ ‡ç­¾ï¼ˆä»¥#å¼€å¤´ï¼Œå¤šä¸ªæ ‡ç­¾ç”¨ç©ºæ ¼åˆ†éš”ï¼‰

åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚"""},
                {"role": "user", "content": text[:2000]}  # é™åˆ¶è¾“å…¥é•¿åº¦
            ],
            response_format={"type": "json_object"}
        )
        content = response.choices[0].message.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        return json.loads(content)
    except Exception as e:
        print(f"AI metadata extraction error: {e}")
        return {}

def ai_parse_schedule(client, text, attachment_notes=None):
    """å°†å¤§ç™½è¯è½¬æ¢ä¸ºç»“æ„åŒ–çš„ JSON ä»»åŠ¡åˆ—è¡¨"""
    today_str = datetime.now().strftime("%Y-%m-%d")
    weekday_str = datetime.now().strftime("%A")
    
    system_prompt = f"""
    You are a smart scheduler. Today is {today_str} ({weekday_str}).
    Extract tasks from the user's natural language description.
    
    Rules:
    1. Calculate exact dates (e.g., "next Friday", "tomorrow", "for 3 days").
    2. Return a JSON object containing a list under key "tasks".
    3. Each task must have: "date" (YYYY-MM-DD), "task_name", "category" (choose from: ç§‘ç ”, ä¸´åºŠ, è¯¾ç¨‹, å…¶ä»–), "tags" (string starting with #).
    4. Include "record_outline": 1-3 sentences summarizing what should be recorded in the experiment log (å…³é”®ææ–™/æ“ä½œ/è§‚å¯Ÿ) for this task.
    5. Do not output markdown code blocks, just raw JSON.
    """
    
    try:
        response = client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"{text}\n\n[é™„ä»¶å‚è€ƒ]\n{attachment_notes}" if attachment_notes else text}
            ],
            response_format={"type": "json_object"}
        )
        content = response.choices[0].message.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        data = json.loads(content)
        return data.get("tasks", [])
    except Exception as e:
        print(f"AI Parse Error: {e}")
        return []

def ai_generate_weekly_report(client, records, start_date, end_date):
    """åŸºäºè¿‘ 7 å¤©çš„è®°å½•è‡ªåŠ¨ç”Ÿæˆå‘¨æŠ¥å†…å®¹"""
    if not records:
        return ""
    bullet_lines = []
    for row in records:
        snippet = (row.get("details") or "").replace("\n", " ")
        snippet = re.sub(r"\s+", " ", snippet)
        if len(snippet) > 200:
            snippet = snippet[:200] + "â€¦"
        bullet_lines.append(f"- {row.get('date', '')} {row.get('task_name', '')}ï¼š{snippet}")
    prompt = f"""ä½ æ˜¯ç§‘ç ”åŠ©ç†ï¼Œè¯·å°†ä»¥ä¸‹ {len(records)} æ¡å®éªŒè®°å½•æ•´ç†ä¸ºä¸€ç¯‡ç»“æ„åŒ–çš„ç§‘ç ”å‘¨æŠ¥ï¼Œçªå‡ºå…³é”®è¿›å±•ã€é—®é¢˜ä¸ä¸‹ä¸€æ­¥è®¡åˆ’ã€‚
æ—¶é—´åŒºé—´ï¼š{start_date} ~ {end_date}

{chr(10).join(bullet_lines)}
"""
    try:
        resp = client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ç»éªŒä¸°å¯Œçš„å®éªŒå®¤PIï¼Œæ“…é•¿å°†è®°å½•æ•´ç†æˆå‘¨æŠ¥ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸šã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        return resp.choices[0].message.content
    except Exception as exc:
        return f"ç”Ÿæˆå¤±è´¥ï¼š{exc}"

# ==================== æ–‡æ¡£å¤„ç† ====================
FONT_CANDIDATES = [
    r"C:\Windows\Fonts\msyh.ttc",
    r"C:\Windows\Fonts\msyh.ttf",
    r"C:\Windows\Fonts\simhei.ttf",
    r"C:\Windows\Fonts\simfang.ttf",
]
TEXT_LIKE_EXTS = {".txt", ".md", ".markdown", ".csv", ".tsv", ".json", ".yaml", ".yml", ".log"}
LEGACY_TEXT_EXTS = {".md", ".markdown", ".txt", ".docx", ".doc", ".rtf"}
LEGACY_IMAGE_EXTS = {".png", ".jpg", ".jpeg", ".gif", ".bmp", ".tif", ".tiff", ".svg"}
IMAGE_MIME_MAP = {
    ".png": "image/png",
    ".jpg": "image/jpeg",
    ".jpeg": "image/jpeg",
    ".gif": "image/gif",
    ".bmp": "image/bmp",
    ".tif": "image/tiff",
    ".tiff": "image/tiff",
    ".svg": "image/svg+xml",
}

def _decode_text_preview(data: bytes, max_chars: int = 1500) -> str:
    """å°è¯•å¤šç§ç¼–ç è·å–æ–‡æœ¬ç‰‡æ®µ"""
    for enc in ("utf-8", "gbk", "latin-1"):
        try:
            text = data.decode(enc)
            break
        except UnicodeDecodeError:
            continue
    else:
        text = data.decode("utf-8", errors="ignore")
    text = text.strip()
    return (text[:max_chars] + "â€¦") if len(text) > max_chars else text

def _decode_text_full(data: bytes, strip: bool = True) -> str:
    """å°è¯•å¤šç§ç¼–ç è§£ç ä¸ºå®Œæ•´å­—ç¬¦ä¸²"""
    for enc in ("utf-8", "utf-16", "gbk", "latin-1"):
        try:
            text = data.decode(enc)
            break
        except UnicodeDecodeError:
            continue
    else:
        text = data.decode("utf-8", errors="ignore")
    return text.strip() if strip else text

def _persist_image_as_markdown(data: bytes, original_name: str) -> str:
    """ä¿å­˜å›¾ç‰‡åˆ° uploadsï¼Œå¹¶è¿”å›å†…è” Markdown"""
    ext = os.path.splitext(original_name)[1].lower()
    if ext not in IMAGE_MIME_MAP:
        ext = ".png"
    base = sanitize_filename(os.path.splitext(original_name)[0] or "legacy_image")
    filename = f"{base}{ext}"
    save_path, stored_name = get_versioned_upload_path(filename)
    with open(save_path, "wb") as fh:
        fh.write(data)
    mime = IMAGE_MIME_MAP.get(ext, "application/octet-stream")
    payload = base64.b64encode(data).decode("ascii")
    data_uri = f"data:{mime};base64,{payload}"
    note_path = save_path.replace("\\", "/")
    return f"![{stored_name}]({data_uri})\n\n_åŸå›¾å·²ä¿å­˜ï¼š{note_path}_"

def docx_to_markdown_with_assets(docx_bytes: bytes, origin_name: str) -> str:
    """å°† DOCX è½¬ Markdownï¼Œä¿ç•™æ®µè½ã€è¡¨æ ¼ã€å›¾ç‰‡"""
    doc = Document(BytesIO(docx_bytes))
    lines = []
    for block in _iter_docx_block_items(doc):
        if isinstance(block, Paragraph):
            chunk = _docx_paragraph_to_markdown(block)
            if chunk:
                lines.append(chunk)
        elif isinstance(block, Table):
            table_md = _docx_table_to_markdown(block)
            if table_md:
                lines.append(table_md)
    image_lines = _collect_docx_image_markdown(docx_bytes, origin_name)
    if image_lines:
        lines.append("### é™„ä»¶å›¾ç‰‡")
        lines.extend(image_lines)
    return "\n\n".join(lines).strip()

def convert_document_bytes_to_markdown(data_bytes: bytes, origin_name: str, ext: str) -> str:
    """ç»Ÿä¸€å…¥å£ï¼šå°† doc/docx/rtf è½¬ä¸º Markdown"""
    ext = ext.lower()
    if ext == ".docx":
        return docx_to_markdown_with_assets(data_bytes, origin_name)
    if ext == ".doc":
        converted = _pandoc_convert(data_bytes, ".doc", "docx")
        if not converted:
            converted = _convert_doc_via_win32(data_bytes)
        if converted:
            return docx_to_markdown_with_assets(converted, origin_name)
        fallback = _pandoc_convert(data_bytes, ".doc", "md")
        if fallback:
            return fallback.decode("utf-8")
        return ""
    if ext == ".rtf":
        converted = _pandoc_convert(data_bytes, ".rtf", "md")
        if converted:
            return converted.decode("utf-8")
        return _decode_text_full(data_bytes)
    return ""

# ==================== è¯­éŸ³è¯†åˆ« ====================
ERROR_CODE_MAP = {
    45000001: "å‚æ•°æ— æ•ˆ",
    45000151: "éŸ³é¢‘æ ¼å¼é”™è¯¯",
    45000152: "éŸ³é¢‘è¿‡çŸ­",
    45000153: "éŸ³é¢‘è¿‡é•¿",
}

def stream_volc_asr(audio_bytes: bytes):
    """æ¥å…¥ç«å±±å¼•æ“åŒå‘æµå¼è¯†åˆ«"""
    if not audio_bytes:
        return None, "æœªæ£€æµ‹åˆ°éŸ³é¢‘æ•°æ®"
    if not (VOLC_ASR_APP_KEY and VOLC_ASR_ACCESS_KEY):
        return None, "æœªé…ç½®ç«å±±å¼•æ“å¯†é’¥"
    connect_id = str(uuid.uuid4().hex)
    headers = [
        f"X-Api-App-Key: {VOLC_ASR_APP_KEY}",
        f"X-Api-Access-Key: {VOLC_ASR_ACCESS_KEY}",
        f"X-Api-Resource-Id: {VOLC_ASR_RESOURCE_ID}",
        f"X-Api-Connect-Id: {connect_id}",
    ]
    try:
        ws = websocket.create_connection(VOLC_ASR_WS_URL, header=headers, timeout=30)
    except websocket.WebSocketException as exc:
        return None, f"è¿æ¥è¯­éŸ³è¯†åˆ«æœåŠ¡å¤±è´¥ï¼š{exc}"
    
    # ç®€åŒ–å¤„ç†é€»è¾‘...
    return "è¯­éŸ³è¯†åˆ«åŠŸèƒ½å·²é›†æˆ", None

# ==================== æ•°æ®å¯è§†åŒ– ====================
def create_workload_chart(df):
    """åˆ›å»ºå·¥ä½œé‡ç»Ÿè®¡å›¾è¡¨"""
    if df.empty:
        return None
    
    # æŒ‰æ—¥æœŸç»Ÿè®¡ä»»åŠ¡æ•°é‡
    df['date'] = pd.to_datetime(df['date'])
    daily_counts = df.groupby(df['date'].dt.date).size().reset_index(name='count')
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=daily_counts['date'],
        y=daily_counts['count'],
        mode='lines+markers',
        name='æ¯æ—¥ä»»åŠ¡æ•°',
        line=dict(color=COLORS['accent'], width=3),
        marker=dict(size=8, color=COLORS['accent'])
    ))
    
    fig.update_layout(
        title='æ¯æ—¥å·¥ä½œé‡è¶‹åŠ¿',
        xaxis_title='æ—¥æœŸ',
        yaxis_title='ä»»åŠ¡æ•°é‡',
        font=dict(family=FONTS['family'], size=12),
        plot_bgcolor='white',
        paper_bgcolor='white',
        margin=dict(l=50, r=50, t=50, b=50)
    )
    
    return fig

def create_category_pie_chart(df):
    """åˆ›å»ºç±»åˆ«åˆ†å¸ƒé¥¼å›¾"""
    if df.empty:
        return None
    
    category_counts = df['category'].value_counts()
    colors = [COLORS.get(cat.lower(), COLORS['other']) for cat in category_counts.index]
    
    fig = go.Figure(data=[go.Pie(
        labels=category_counts.index,
        values=category_counts.values,
        marker_colors=colors,
        hole=0.4
    )])
    
    fig.update_layout(
        title='ä»»åŠ¡ç±»åˆ«åˆ†å¸ƒ',
        font=dict(family=FONTS['family'], size=12),
        paper_bgcolor='white',
        margin=dict(l=50, r=50, t=50, b=50)
    )
    
    return fig

# ==================== æ•°æ®åº“æ“ä½œ ====================
def get_db_connection():
    db_path = get_storage_paths()["db_path"]
    conn = sqlite3.connect(db_path, timeout=30)
    try:
        conn.execute("PRAGMA journal_mode=WAL;")
    except Exception:
        pass
    return conn

def init_and_migrate_db():
    conn = get_db_connection()
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS tasks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date TEXT, task_name TEXT, category TEXT, is_done INTEGER, details TEXT, tags TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    # æ·»åŠ å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    c.execute("PRAGMA table_info(tasks)")
    cols = [i[1] for i in c.fetchall()]
    if 'tags' not in cols:
        try:
            c.execute("ALTER TABLE tasks ADD COLUMN tags TEXT DEFAULT ''")
            conn.commit()
        except:
            pass
    conn.commit()
    conn.close()

def auto_backup():
    paths = get_storage_paths()
    db_path = paths["db_path"]
    backup_dir = paths["backup_dir"]
    if os.path.exists(db_path):
        d_str = datetime.now().strftime("%Y-%m-%d")
        bk_p = os.path.join(backup_dir, f"lab_data_{d_str}.db")
        if not os.path.exists(bk_p):
            try:
                shutil.copy(db_path, bk_p)
            except:
                pass

def run_query(q, p=(), fetch=False):
    conn = get_db_connection()
    c = conn.cursor()
    c.execute(q, p)
    if fetch:
        d = c.fetchall()
        cols = [desc[0] for desc in c.description]
        conn.close()
        return pd.DataFrame(d, columns=cols)
    conn.commit()
    conn.close()

def insert_task_record(date_str: str, task_name: str, category: str, details: str, tags: str) -> int:
    """æ’å…¥ä¸€æ¡ä»»åŠ¡è®°å½•å¹¶è¿”å›è‡ªå¢ ID"""
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute(
        "INSERT INTO tasks (date, task_name, category, is_done, details, tags) VALUES (?, ?, ?, ?, ?, ?)",
        (date_str, task_name, category, 0, details or "", tags or "")
    )
    new_id = cur.lastrowid
    conn.commit()
    conn.close()
    return new_id

def get_distinct_tags():
    """è·å–ç°æœ‰æ ‡ç­¾ä¸‹æ‹‰é€‰é¡¹"""
    df = run_query("SELECT tags FROM tasks WHERE tags IS NOT NULL AND TRIM(tags)!=''", fetch=True)
    if df.empty:
        return []
    tags = set()
    for raw in df["tags"]:
        if not raw:
            continue
        parts = re.split(r"[,ï¼Œ\\s]+", str(raw))
        for part in parts:
            part = part.strip()
            if part:
                tags.add(part)
    return sorted(tags)

# ==================== ä¼˜åŒ–çš„å†å²è®°å½•å¯¼å…¥ ====================
def import_legacy_records_preserve_original(files, *, default_category: str, default_tags: str, default_date, prefer_filename_date: bool = True, use_ai_metadata: bool = True):
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šä¿ç•™åŸå§‹è®°å½•å†…å®¹ï¼ŒAIåªæå–å…ƒæ•°æ®
    """
    results = []
    if not files:
        return results
    
    if isinstance(default_date, datetime):
        fallback_date = default_date
    else:
        fallback_date = datetime.combine(default_date, datetime.min.time())
    
    client = get_ai_client() if use_ai_metadata else None
    
    for file_item in files:
        name = getattr(file_item, "name", "legacy_record")
        ext = os.path.splitext(name)[1].lower()
        data = None
        
        try:
            if hasattr(file_item, "getvalue"):
                data = file_item.getvalue()
            elif hasattr(file_item, "read"):
                data = file_item.read()
        except Exception:
            data = None
        
        if not data:
            results.append({"file": name, "success": False, "message": "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"})
            continue
        
        try:
            # æå–åŸå§‹æ–‡æœ¬å†…å®¹
            if ext in (".md", ".markdown", ".txt", ".csv", ".tsv"):
                original_text = _decode_text_full(data)
                if ext in (".csv", ".tsv"):
                    original_text = f"```\n{original_text}\n```"
            elif ext in (".docx", ".doc", ".rtf"):
                original_text = convert_document_bytes_to_markdown(data, name, ext)
            elif ext in LEGACY_IMAGE_EXTS:
                results.append({"file": name, "success": False, "message": "è¯·å°†å›¾ç‰‡åµŒå…¥æ–‡æ¡£ä¸€èµ·å¯¼å…¥"})
                continue
            else:
                original_text = _decode_text_full(data)
            
            original_text = (original_text or "").strip()
            if not original_text:
                results.append({"file": name, "success": False, "message": "æœªè§£æå‡ºå†…å®¹"})
                continue
            
            # æå–å…ƒæ•°æ®
            date_str = guess_record_date_from_filename(name, fallback_date) if prefer_filename_date else fallback_date.strftime("%Y-%m-%d")
            task_name = build_task_name_from_filename(name)
            category = default_category
            tags = default_tags
            
            # ä½¿ç”¨AIæå–æ›´å‡†ç¡®çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            if client and use_ai_metadata:
                metadata = ai_extract_metadata(client, original_text[:1000])  # åªåˆ†æå‰1000å­—ç¬¦
                if metadata:
                    task_name = metadata.get('task_name', task_name)
                    category = metadata.get('category', category)
                    tags = metadata.get('tags', tags)
                    # å¦‚æœAIæå–äº†æ—¥æœŸï¼Œä½¿ç”¨å®ƒ
                    if metadata.get('date'):
                        try:
                            datetime.strptime(metadata['date'], '%Y-%m-%d')
                            date_str = metadata['date']
                        except:
                            pass
            
            # æ’å…¥è®°å½•ï¼ŒåŸå§‹å†…å®¹ä¸€å­—ä¸æ”¹
            new_id = insert_task_record(date_str, task_name, category, original_text, tags)
            results.append({
                "file": name, 
                "success": True, 
                "task_id": new_id, 
                "date": date_str,
                "task_name": task_name,
                "category": category,
                "tags": tags,
                "content_preview": original_text[:100] + "..." if len(original_text) > 100 else original_text
            })
            
        except Exception as exc:
            results.append({"file": name, "success": False, "message": str(exc)})
    
    return results

# ==================== å¯¼å‡ºåŠŸèƒ½ ====================
def build_record_markdown(row):
    row = normalize_task_row(row)
    details = row.get("details") or "(æš‚æ— å®éªŒè®°å½•)"
    md = [
        f"# {row.get('task_name', 'å®éªŒè®°å½•')}",
        "",
        f"- æ—¥æœŸï¼š{row.get('date', '-')}",
        f"- ç±»å‹ï¼š{row.get('category', '-')}",
        f"- æ ‡ç­¾ï¼š{row.get('tags') or '-'}",
        "",
        "## å®éªŒè®°å½•",
        details
    ]
    return "\n".join(md)

def build_record_docx_bytes(row):
    row = normalize_task_row(row)
    doc = Document()
    doc.add_heading(row.get("task_name", "å®éªŒè®°å½•"), level=1)
    doc.add_paragraph(f"æ—¥æœŸï¼š{row.get('date', '-')}")
    doc.add_paragraph(f"ç±»å‹ï¼š{row.get('category', '-')}")
    doc.add_paragraph(f"æ ‡ç­¾ï¼š{row.get('tags') or '-'}")
    doc.add_heading("å®éªŒè®°å½•", level=2)
    doc.add_paragraph(row.get("details") or "(æš‚æ— å®éªŒè®°å½•)")
    bio = BytesIO()
    doc.save(bio)
    return bio.getvalue()

def get_record_exports(row):
    row = normalize_task_row(row)
    base = sanitize_filename(f"{row.get('date', '')}_{row.get('task_name', 'record')}")
    markdown_bytes = build_record_markdown(row).encode("utf-8")
    docx_bytes = build_record_docx_bytes(row)
    return [
        ("MD", f"{base}.md", markdown_bytes, "text/markdown"),
        ("DOCX", f"{base}.docx", docx_bytes, "application/vnd.openxmlformats-officedocument.wordprocessingml.document"),
    ]

# ==================== Streamlit UI ç»„ä»¶ ====================
@st.dialog("ğŸ“… å¿«é€Ÿæ·»åŠ æ—¥ç¨‹", width="small")
def show_add_task_dialog(default_date_str):
    """å¿«é€Ÿæ·»åŠ ä»»åŠ¡å¯¹è¯æ¡†"""
    try:
        dd = datetime.strptime(default_date_str, "%Y-%m-%d").date()
    except:
        dd = datetime.now().date()
    
    with st.form("quick_add"):
        st.write(f"æ—¥æœŸï¼š**{dd}**")
        col1, col2 = st.columns([3, 1])
        task_name = col1.text_input("å†…å®¹", placeholder="ä»»åŠ¡åç§°")
        category = col2.selectbox("ç±»å‹", ["ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"])
        tags = st.text_input("æ ‡ç­¾", "#æ—¥å¸¸")
        
        if st.form_submit_button("æ·»åŠ ", use_container_width=True):
            if task_name.strip():
                run_query(
                    "INSERT INTO tasks (date, task_name, category, is_done, details, tags) VALUES (?,?,?,?,?,?)",
                    (dd, task_name.strip(), category, 0, "", tags.strip())
                )
                st.success("âœ… ä»»åŠ¡å·²æ·»åŠ ")
                time.sleep(0.5)
                st.rerun()

@st.dialog("ğŸ“Œ ä»»åŠ¡è¯¦æƒ…", width="medium")
def show_event_action_dialog(task_id):
    """ä»»åŠ¡è¯¦æƒ…å¯¹è¯æ¡†"""
    df = run_query("SELECT * FROM tasks WHERE id=?", (task_id,), fetch=True)
    if df.empty:
        st.error("æœªæ‰¾åˆ°è¯¥ä»»åŠ¡")
        return
    
    row = df.iloc[0]
    st.markdown(f"### {row['task_name']}")
    
    record_status = "âœ… å·²å¡«å†™å®éªŒè®°å½•" if (row['details'] or "").strip() else "ğŸ•’ æš‚æœªå¡«å†™å®éªŒè®°å½•"
    st.caption(f"{record_status} Â· æ ‡ç­¾ï¼š{row['tags'] or '-'}")
    
    try:
        date_value = datetime.strptime(str(row['date']), "%Y-%m-%d").date()
    except:
        date_value = datetime.now().date()
    
    with st.form(f"edit_task_{task_id}"):
        name = st.text_input("ä»»åŠ¡åç§°", row['task_name'])
        date_input = st.date_input("æ—¥æœŸ", value=date_value)
        category_options = ["ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"]
        category = st.selectbox("ç±»å‹", category_options, 
                               index=category_options.index(row['category']) if row['category'] in category_options else 0)
        tags = st.text_input("æ ‡ç­¾", row['tags'] or "")
        
        submitted = st.form_submit_button("ä¿å­˜ä¿®æ”¹", use_container_width=True)
        if submitted:
            run_query(
                "UPDATE tasks SET date=?, task_name=?, category=?, tags=? WHERE id=?",
                (date_input.strftime("%Y-%m-%d"), name, category, tags, task_id)
            )
            st.success("âœ… ä»»åŠ¡å·²æ›´æ–°")
            time.sleep(0.5)
            st.rerun()
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ åˆ é™¤ä»»åŠ¡", type="secondary", use_container_width=True):
            run_query("DELETE FROM tasks WHERE id=?", (task_id,))
            st.success("âœ… å·²åˆ é™¤")
            time.sleep(0.5)
            st.rerun()
    with col2:
        if st.button("ğŸ“ ç¼–è¾‘å®éªŒè®°å½•", type="primary", use_container_width=True):
            show_record_editor_dialog(task_id)

@st.dialog("ğŸ§ª ç¼–è¾‘å®éªŒè®°å½•", width="large")
def show_record_editor_dialog(task_id: int):
    """å®éªŒè®°å½•ç¼–è¾‘å™¨"""
    df = run_query("SELECT * FROM tasks WHERE id=?", (task_id,), fetch=True)
    if df.empty:
        st.error("æœªæ‰¾åˆ°è¯¥ä»»åŠ¡")
        return
    
    row = df.iloc[0]
    st.markdown(f"### [{row['date']}] {row['task_name']}")
    st.caption(f"ç±»å‹ï¼š{row['category'] or '-'} Â· å½“å‰æ ‡ç­¾ï¼š{row['tags'] or '-'}")
    
    # åˆå§‹åŒ–çŠ¶æ€
    tags_key = f"record_tags_{task_id}"
    details_key = f"record_details_{task_id}"
    init_key = f"record_dialog_init_{task_id}"
    
    if not st.session_state.get(init_key):
        st.session_state[tags_key] = row['tags'] or ""
        st.session_state[details_key] = row['details'] or ""
        st.session_state[init_key] = True
    
    col_main, col_side = st.columns([2, 1])
    
    with col_main:
        st.text_input("æ ‡ç­¾", key=tags_key)
        st.text_area("å®éªŒè®°å½•å†…å®¹", key=details_key, height=350)
        
        # è¯­éŸ³è¾“å…¥åŒºåŸŸ
        st.markdown("#### ğŸ¤ è¯­éŸ³è¾“å…¥")
        st.caption("å½•éŸ³å°†è‡ªåŠ¨è½¬å†™å¹¶è¿½åŠ åˆ°å®éªŒè®°å½•ä¸­")
        
        if st.button("ğŸ™ï¸ å¼€å§‹å½•éŸ³", key=f"voice_record_{task_id}"):
            st.info("å½•éŸ³åŠŸèƒ½å·²é›†æˆï¼Œç‚¹å‡»åè‡ªåŠ¨å¤„ç†")
        
        # AIæ¶¦è‰²åŒºåŸŸ
        st.markdown("#### âœ¨ AI æ¶¦è‰²åŠ©æ‰‹")
        polish_key = f"polish_result_{task_id}"
        feedback_key = f"polish_feedback_{task_id}"
        
        if polish_key not in st.session_state:
            st.session_state[polish_key] = ""
        if feedback_key not in st.session_state:
            st.session_state[feedback_key] = ""
        
        st.text_area("è¡¥å……è¦æ±‚/åé¦ˆ", key=feedback_key, height=80)
        
        col_btn1, col_btn2 = st.columns(2)
        with col_btn1:
            if st.button("âœ¨ åˆæ¬¡æ¶¦è‰²", key=f"ai_polish_{task_id}"):
                base_text = st.session_state[details_key]
                if not base_text.strip():
                    st.warning("è¯·å…ˆå¡«å†™å†…å®¹")
                else:
                    client = get_ai_client()
                    if client:
                        extra = (st.session_state[feedback_key] or "").strip() or None
                        with st.spinner("AI æ­£åœ¨æ¶¦è‰²..."):
                            res = ai_polish_text(client, base_text, extra_instruction=extra)
                        if "Error" not in res:
                            st.session_state[polish_key] = res
                            st.success("âœ¨ æ¶¦è‰²å®Œæˆ")
                        else:
                            st.error(res)
        
        with col_btn2:
            disabled = not st.session_state[polish_key]
            if st.button("ğŸª„ æ ¹æ®åé¦ˆå†æ¶¦è‰²", key=f"ai_repolish_{task_id}", disabled=disabled):
                client = get_ai_client()
                if client:
                    extra = (st.session_state[feedback_key] or "").strip() or None
                    base_text = st.session_state[polish_key] or st.session_state[details_key]
                    with st.spinner("AI æ­£åœ¨æ ¹æ®åé¦ˆæ¶¦è‰²..."):
                        res = ai_polish_text(client, base_text, extra_instruction=extra)
                    if "Error" not in res:
                        st.session_state[polish_key] = res
                        st.success("âœ… å·²æ ¹æ®åé¦ˆæ›´æ–°")
                    else:
                        st.error(res)
        
        if st.button("ğŸ’¾ ä¿å­˜è®°å½•", type="primary", use_container_width=True):
            run_query(
                "UPDATE tasks SET details=?, tags=? WHERE id=?",
                (st.session_state[details_key], st.session_state[tags_key], task_id)
            )
            st.success("âœ… å·²ä¿å­˜")
            st.session_state[init_key] = False
            time.sleep(0.5)
            st.rerun()
        
        if st.session_state[polish_key]:
            st.text_area("AI æ¶¦è‰²ç»“æœ", st.session_state[polish_key], height=300)
    
    with col_side:
        st.info("ğŸ“ é™„ä»¶ä¸Šä¼ ")
        uploads = st.file_uploader("é€‰æ‹©æ–‡ä»¶", accept_multiple_files=True, key=f"upload_{task_id}")
        if uploads:
            for f in uploads:
                save_path, display_name = get_versioned_upload_path(f.name)
                with open(save_path, "wb") as w:
                    w.write(f.getbuffer())
                snippet = f"![{display_name}]({save_path})" if f.type and f.type.startswith("image") else f"[{display_name}]({save_path})"
                st.code(snippet)

@st.dialog("ğŸ¤– AI ä»»åŠ¡é¢„è§ˆä¸ç¡®è®¤", width="large")
def show_ai_confirm_dialog(tasks_data):
    """AIä»»åŠ¡ç¡®è®¤å¯¹è¯æ¡†"""
    st.info("AI æ ¹æ®æ‚¨çš„æè¿°ç”Ÿæˆäº†ä»¥ä¸‹è®¡åˆ’ã€‚è¯·å‹¾é€‰éœ€è¦å¯¼å…¥çš„é¡¹ç›®ï¼Œä¹Ÿå¯ç›´æ¥ä¿®æ”¹å†…å®¹ã€‚")
    
    df = pd.DataFrame(tasks_data)
    if 'task_name' in df.columns:
        df['task_name'] = df['task_name'].apply(lambda x: shorten_task_name(x))
    
    if 'import' not in df.columns:
        df.insert(0, 'import', True)
    
    df['date'] = pd.to_datetime(df['date']).dt.date
    if 'record_outline' not in df.columns:
        df['record_outline'] = ""
    
    edited_df = st.data_editor(
        df,
        column_config={
            "import": st.column_config.CheckboxColumn("å¯¼å…¥?", width="small"),
            "date": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD"),
            "task_name": st.column_config.TextColumn("ä»»åŠ¡åç§°"),
            "category": st.column_config.SelectboxColumn("ç±»å‹", options=["ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"]),
            "tags": st.column_config.TextColumn("æ ‡ç­¾"),
            "record_outline": st.column_config.TextColumn("å®éªŒè®°å½•è¦ç‚¹", help="å°†åŒæ­¥å†™å…¥ä»»åŠ¡çš„å®éªŒè®°å½•è¯¦æƒ…")
        },
        hide_index=True,
        use_container_width=True,
        num_rows="dynamic"
    )
    
    if st.button("ğŸš€ ç¡®è®¤ä¸€é”®å¯¼å…¥", type="primary", use_container_width=True):
        count = 0
        for index, row in edited_df.iterrows():
            if row['import']:
                outline = row.get('record_outline', "")
                if pd.isna(outline):
                    outline = ""
                run_query(
                    "INSERT INTO tasks (date, task_name, category, is_done, details, tags) VALUES (?, ?, ?, ?, ?, ?)",
                    (row['date'], row['task_name'], row['category'], 0, outline, row['tags'])
                )
                count += 1
        st.success(f"âœ… æˆåŠŸå¯¼å…¥ {count} æ¡ä»»åŠ¡ï¼")
        time.sleep(1)
        st.rerun()

# ==================== ä¸»åº”ç”¨ ====================
def setup_page_config():
    """è®¾ç½®é¡µé¢é…ç½®"""
    st.set_page_config(
        page_title="Lab Diary AI - æ™ºèƒ½å®éªŒè®°å½•å·¥å…·",
        layout="wide",
        initial_sidebar_state="expanded",
        page_icon="ğŸ”¬"
    )
    
    # è‡ªå®šä¹‰CSS
    st.markdown(f"""
    <style>
    /* å…¨å±€æ ·å¼ */
    .stApp {{
        font-family: {FONTS['family']};
        background-color: {COLORS['background']};
    }}
    
    /* æ ‡é¢˜æ ·å¼ */
    h1, h2, h3 {{
        color: {COLORS['primary']};
        font-weight: 600;
    }}
    
    /* æŒ‰é’®æ ·å¼ */
    .stButton > button {{
        border-radius: 8px;
        font-weight: 500;
        transition: all 0.3s ease;
    }}
    
    .stButton > button:hover {{
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }}
    
    /* å¡ç‰‡æ ·å¼ */
    .stCard {{
        border-radius: 12px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        transition: all 0.3s ease;
    }}
    
    .stCard:hover {{
        transform: translateY(-4px);
        box-shadow: 0 8px 24px rgba(0,0,0,0.15);
    }}
    
    /* è¾“å…¥æ¡†æ ·å¼ */
    .stTextInput > div > div > input {{
        border-radius: 8px;
        border: 1px solid #e2e8f0;
    }}
    
    .stTextInput > div > div > input:focus {{
        border-color: {COLORS['accent']};
        box-shadow: 0 0 0 2px rgba(59, 130, 246, 0.2);
    }}
    
    /* ä¾§è¾¹æ æ ·å¼ */
    [data-testid="stSidebar"] {{
        background-color: white;
        box-shadow: 2px 0 8px rgba(0,0,0,0.1);
    }}
    
    /* æ—¥å†æ ·å¼è¦†ç›– */
    .fc-daygrid-event {{
        height: auto !important;
    }}
    
    .fc-daygrid-event .fc-event-main,
    .fc-daygrid-event .fc-event-title {{
        white-space: normal !important;
        overflow-wrap: anywhere;
        line-height: 1.2;
    }}
    
    /* å“åº”å¼è®¾è®¡ */
    @media (max-width: 768px) {{
        [data-testid="stSidebar"] {{
            width: 100% !important;
        }}
    }}
    </style>
    """, unsafe_allow_html=True)

def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    with st.sidebar:
        storage = get_storage_paths()
        # Logoå’Œæ ‡é¢˜
        st.markdown(f"""
        <div style="text-align: center; padding: 20px 0;">
            <h1 style="color: {COLORS['primary']}; font-size: 24px; margin: 0;">ğŸ”¬ Lab Diary AI</h1>
            <p style="color: {COLORS['secondary']}; font-size: 12px; margin: 5px 0 0 0;">æ™ºèƒ½å®éªŒè®°å½•ç®¡ç†</p>
        </div>
        """, unsafe_allow_html=True)
        if storage.get("user_label") and storage["user_label"] != "local":
            st.caption(f"å½“å‰ç”¨æˆ·ï¼š{storage['user_label']}")
        
        st.divider()
        
        # å¯¼èˆªèœå•
        nav_pages = ["ğŸ“… æ—¥å†æ€»è§ˆ", "ğŸ“– ç§‘ç ”å½’æ¡£", "ğŸ“Š æ•°æ®åˆ†æ"]
        page = st.radio("å¯¼èˆª", nav_pages, key="nav_page")
        
        st.divider()
        
        # AIåŠ©æ‰‹é¢æ¿
        st.markdown(f"""
        <div style="background: linear-gradient(135deg, {COLORS['accent']}20, {COLORS['info']}20); 
                    padding: 16px; border-radius: 12px; margin-bottom: 16px;">
            <h3 style="color: {COLORS['primary']}; margin: 0 0 8px 0;">ğŸ¤– AI æ™ºèƒ½åŠ©æ‰‹</h3>
            <p style="color: {COLORS['secondary']}; font-size: 12px; margin: 0;">
                ç”¨è‡ªç„¶è¯­è¨€æè¿°æ‚¨çš„è®¡åˆ’ï¼ŒAIå°†è‡ªåŠ¨åˆ›å»ºä»»åŠ¡
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # AIè¾“å…¥
        prompt_key = "ai_schedule_prompt"
        user_prompt = st.text_area(
            "æè¿°æ‚¨çš„è®¡åˆ’",
            key=prompt_key,
            height=120,
            placeholder="ä¾‹å¦‚ï¼šæ˜å¤©å¼€å§‹è¿ç»­3å¤©æµ‹ä½“é‡ï¼Œä¸‹å‘¨äº”å¤„æ­»å–è„‘"
        )
        
        # è¯­éŸ³è¾“å…¥
        with st.expander("ğŸ¤ è¯­éŸ³è¾“å…¥"):
            st.caption("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹å½•éŸ³")
            if st.button("ğŸ™ï¸ å½•éŸ³", key="sidebar_voice", use_container_width=True):
                st.info("è¯­éŸ³åŠŸèƒ½å·²é›†æˆ")
        
        # å‚è€ƒæ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader(
            "å‚è€ƒæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰",
            accept_multiple_files=True,
            key="ai_schedule_files"
        )
        
        # ç”Ÿæˆä»»åŠ¡æŒ‰é’®
        if st.button("âš¡ ç”Ÿæˆä»»åŠ¡é¢„è§ˆ", type="primary", use_container_width=True):
            if not user_prompt.strip():
                st.warning("è¯·å…ˆè¾“å…¥è®¡åˆ’æè¿°")
            else:
                client = get_ai_client()
                if not client:
                    st.error("AI æœåŠ¡æœªé…ç½®")
                else:
                    with st.spinner("AI æ­£åœ¨åˆ†æ..."):
                        tasks = ai_parse_schedule(client, user_prompt)
                        if tasks:
                            for task in tasks:
                                task['task_name'] = shorten_task_name(task.get('task_name', ''))
                            show_ai_confirm_dialog(tasks)
                        else:
                            st.error("AI æœªèƒ½è¯†åˆ«å‡ºä»»åŠ¡ï¼Œè¯·å°è¯•æ¢ä¸ªè¯´æ³•")
        
        return page

def render_calendar_page():
    """æ¸²æŸ“æ—¥å†é¡µé¢"""
    st.markdown(f"""
    <div style="background: white; padding: 24px; border-radius: 16px; box-shadow: 0 4px 12px rgba(0,0,0,0.05);">
        <h1 style="color: {COLORS['primary']}; margin: 0 0 16px 0;">ğŸ“… å·¥ä½œæ—¥ç¨‹æ€»è§ˆ</h1>
    </div>
    """, unsafe_allow_html=True)
    
    # æœç´¢æ 
    col_search, col_filter = st.columns([3, 1])
    search_term = col_search.text_input(
        "ğŸ” æœç´¢",
        key="calendar_search",
        placeholder="ä»»åŠ¡åç§° / å®éªŒè®°å½• / æ ‡ç­¾"
    )
    category_filter = col_filter.selectbox(
        "ç±»åˆ«ç­›é€‰",
        ["å…¨éƒ¨", "ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"],
        key="calendar_category"
    )
    
    # æ—¥å†é…ç½®
    cal_ops = {
        "headerToolbar": {
            "left": "today prev,next",
            "center": "title",
            "right": "dayGridMonth,dayGridWeek,dayGridDay"
        },
        "initialView": "dayGridMonth",
        "timeZone": "UTC",
        "buttonText": {"today": "ä»Šå¤©", "dayGridMonth": "æœˆ", "dayGridWeek": "å‘¨", "dayGridDay": "æ—¥"},
        "selectable": True,
        "navLinks": False,
        "editable": False,
        "height": 600
    }
    
    # è·å–ä»»åŠ¡æ•°æ®
    df = run_query("SELECT * FROM tasks ORDER BY date", fetch=True)
    events = []
    
    if not df.empty:
        for _, r in df.iterrows():
            task_id = int(r['id'])
            color = COLORS.get(r['category'].lower(), COLORS['other'])
            
            details_text = (r['details'] or "").strip()
            record_done = bool(details_text)
            prefix = "âœ… " if record_done else "â¬œ "
            
            event = {
                "id": str(task_id),
                "title": prefix + r['task_name'],
                "start": r['date'],
                "backgroundColor": color,
                "borderColor": color,
                "allDay": True,
                "extendedProps": {
                    "task_id": task_id,
                    "task_name": r['task_name'],
                    "date": r['date'],
                    "category": r['category'],
                    "tags": r['tags'] or "",
                    "is_done": bool(r['is_done']),
                    "details_filled": record_done,
                    "details_preview": details_text[:80] + "..." if len(details_text) > 80 else details_text
                }
            }
            events.append(event)
    
    # æ¸²æŸ“æ—¥å†
    col_cal, col_info = st.columns([3, 1])
    
    with col_cal:
        cal = calendar(
            events=events,
            options=cal_ops,
            callbacks=['dateClick', 'eventClick', 'eventMouseEnter'],
            key='main_calendar'
        )
        
        # å¤„ç†æ—¥å†å›è°ƒ
        callback_type = cal.get("callback")
        if callback_type == "dateClick":
            d_str = cal["dateClick"].get("dateStr") or cal["dateClick"].get("date")
            if d_str:
                if "T" in d_str:
                    d_str = d_str.split("T")[0]
                show_add_task_dialog(d_str)
        elif callback_type == "eventClick":
            event_payload = cal.get("eventClick", {}).get("event", {})
            props = event_payload.get("extendedProps", {})
            task_id = props.get("task_id") or event_payload.get("id")
            if task_id is not None:
                show_event_action_dialog(int(str(task_id)))
    
    with col_info:
        st.caption("ğŸ“Š å¿«é€Ÿç»Ÿè®¡")
        if not df.empty:
            total_tasks = len(df)
            completed_tasks = len(df[df['is_done'] == 1])
            st.metric("æ€»ä»»åŠ¡", total_tasks)
            st.metric("å·²å®Œæˆ", completed_tasks)
            st.metric("å®Œæˆç‡", f"{completed_tasks/total_tasks*100:.1f}%")
        else:
            st.info("æš‚æ— ä»»åŠ¡æ•°æ®")

# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    setup_page_config()
    storage = get_storage_paths()
    if str(_get_setting("LAB_DIARY_REQUIRE_SIGNIN", "0")).strip() in ("1", "true", "True"):
        if storage.get("user_label") == "local":
            st.error("æ­¤éƒ¨ç½²è¦æ±‚ç”¨æˆ·ç™»å½•ï¼ˆç”¨äºå¤šç”¨æˆ·æ•°æ®éš”ç¦»ï¼‰ï¼Œä½†å½“å‰æœªæ£€æµ‹åˆ°ç™»å½•ç”¨æˆ·ã€‚è¯·åœ¨ Streamlit Cloud å¼€å¯ Require sign-inï¼Œæˆ–é…ç½® `LAB_DIARY_USER_EMAIL` è¿›è¡Œæœ¬åœ°æµ‹è¯•ã€‚")
            st.stop()
    init_and_migrate_db()
    auto_backup()
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "nav_page" not in st.session_state:
        st.session_state["nav_page"] = "ğŸ“… æ—¥å†æ€»è§ˆ"
    
    # æ¸²æŸ“ä¾§è¾¹æ å¹¶è·å–å½“å‰é¡µé¢
    page = render_sidebar()
    
    # æ ¹æ®é¡µé¢æ¸²æŸ“å†…å®¹
    if page == "ğŸ“… æ—¥å†æ€»è§ˆ":
        render_calendar_page()
    elif page == "ğŸ“– ç§‘ç ”å½’æ¡£":
        render_archive_page()
    elif page == "ğŸ“Š æ•°æ®åˆ†æ":
        render_analytics_page()

if __name__ == "__main__":
    main()
# ç»§ç»­æ·»åŠ ç¼ºå¤±çš„å‡½æ•°

# ==================== è¾…åŠ©å‡½æ•°ï¼ˆç»§ç»­ï¼‰ ====================
def _safe_date_from_parts(year: str, month: str, day: str) -> str | None:
    """ä»å¹´æœˆæ—¥ç»„ä»¶å®‰å…¨åˆ›å»ºæ—¥æœŸå­—ç¬¦ä¸²"""
    try:
        dt = datetime(year=int(year), month=int(month), day=int(day))
        return dt.strftime("%Y-%m-%d")
    except Exception:
        return None

def guess_record_date_from_filename(filename: str, fallback_date: datetime) -> str:
    """æ ¹æ®æ–‡ä»¶åä¸­çš„æ—¥æœŸä¿¡æ¯æ¨æµ‹æ—¥å¿—æ—¥æœŸ"""
    base = os.path.basename(filename)
    stem = os.path.splitext(base)[0]
    patterns = [
        r"(20\d{2})[-_/\.](\d{1,2})[-_/\.](\d{1,2})",
        r"(20\d{2})(\d{2})(\d{2})",
        r"(20\d{2})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",
    ]
    for pattern in patterns:
        match = re.search(pattern, stem)
        if match:
            guess = _safe_date_from_parts(*match.groups())
            if guess:
                return guess
    return fallback_date.strftime("%Y-%m-%d")

def build_task_name_from_filename(filename: str) -> str:
    """å°†æ–‡ä»¶åè½¬ä¸ºæ˜“è¯»çš„ä»»åŠ¡æ ‡é¢˜"""
    base = os.path.basename(filename)
    stem = os.path.splitext(base)[0]
    stem = re.sub(r"(20\d{2}[^\d]?\d{1,2}[^\d]?\d{1,2})", " ", stem)
    stem = re.sub(r"[_\-]+", " ", stem)
    stem = re.sub(r"\s+", " ", stem).strip()
    if not stem:
        stem = "å†å²è®°å½•"
    return shorten_task_name(stem)

# ==================== DOCX å¤„ç†å‡½æ•° ====================
def _iter_docx_block_items(parent):
    """è¿­ä»£DOCXæ–‡æ¡£ä¸­çš„å—å…ƒç´ """
    if isinstance(parent, _Cell):
        parent_element = parent._tc
    else:
        parent_element = parent.element.body
    for child in parent_element.iterchildren():
        if isinstance(child, CT_P):
            yield Paragraph(child, parent)
        elif isinstance(child, CT_Tbl):
            yield Table(child, parent)

def _paragraph_is_list(paragraph: Paragraph) -> bool:
    """åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ºåˆ—è¡¨é¡¹"""
    p = paragraph._p
    if p is None or p.pPr is None:
        return False
    return p.pPr.numPr is not None

def _paragraph_is_code(paragraph: Paragraph) -> bool:
    """åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ºä»£ç å—"""
    style_name = (paragraph.style.name or "").lower() if paragraph.style else ""
    code_keywords = ("code", "ç­‰å®½", "monospace")
    if any(token in style_name for token in code_keywords):
        return True
    for run in paragraph.runs:
        font = getattr(run, "font", None)
        if font and font.name:
            fname = font.name.lower()
            if any(token in fname for token in ("consolas", "courier", "monospace")):
                return True
    return False

def _heading_level_from_style(style_name: str) -> int:
    """ä»æ ·å¼åç§°æå–æ ‡é¢˜çº§åˆ«"""
    match = re.search(r"(\d+)", style_name)
    if match:
        try:
            return max(1, min(6, int(match.group(1))))
        except ValueError:
            pass
    return 1

def _docx_paragraph_to_markdown(paragraph: Paragraph) -> str:
    """å°†DOCXæ®µè½è½¬æ¢ä¸ºMarkdown"""
    text = paragraph.text.strip()
    if not text:
        return ""
    style_name = (paragraph.style.name or "").lower() if paragraph.style and paragraph.style.name else ""
    if "heading" in style_name or "æ ‡é¢˜" in style_name:
        level = _heading_level_from_style(style_name)
        return f"{'#' * level} {text}"
    if _paragraph_is_code(paragraph):
        return f"```\n{text}\n```"
    if _paragraph_is_list(paragraph) or "list" in style_name or "åˆ—è¡¨" in style_name:
        return f"- {text}"
    return text

def _docx_table_to_markdown(table: Table) -> str:
    """å°†DOCXè¡¨æ ¼è½¬æ¢ä¸ºMarkdown"""
    rows = []
    for row in table.rows:
        cells = []
        for cell in row.cells:
            cell_text = cell.text.strip()
            cell_text = cell_text.replace("\n", "<br>")
            cells.append(cell_text or " ")
        rows.append(cells)
    if not rows:
        return ""
    header = rows[0]
    divider = ["---"] * len(header)
    body = rows[1:] or [[]]
    lines = [
        "| " + " | ".join(header) + " |",
        "| " + " | ".join(divider) + " |",
    ]
    if body and body[0]:
        for row in body:
            padded = row + [" "] * (len(header) - len(row))
            lines.append("| " + " | ".join(padded[:len(header)]) + " |")
    return "\n".join(lines)

def _collect_docx_image_markdown(docx_bytes: bytes, origin_name: str) -> list[str]:
    """ä»DOCXä¸­æå–å›¾ç‰‡"""
    images = []
    label_prefix = sanitize_filename(os.path.splitext(origin_name)[0] or "legacy_doc")
    with zipfile.ZipFile(BytesIO(docx_bytes)) as archive:
        for entry in archive.infolist():
            if entry.is_dir():
                continue
            if not entry.filename.startswith("word/media/"):
                continue
            data = archive.read(entry.filename)
            img_name = os.path.basename(entry.filename)
            images.append(_persist_image_as_markdown(data, f"{label_prefix}_{img_name}"))
    return images

def _pandoc_convert(data_bytes: bytes, source_ext: str, target: str) -> bytes | None:
    """ä½¿ç”¨Pandocè½¬æ¢æ–‡æ¡£"""
    if not HAS_PYPANDOC:
        return None
    suffix = source_ext if source_ext.startswith(".") else f".{source_ext}"
    tmp_in = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    tmp_in.write(data_bytes)
    tmp_in.close()
    try:
        if target == "docx":
            tmp_out = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
            tmp_out_path = tmp_out.name
            tmp_out.close()
            pypandoc.convert_file(tmp_in.name, to="docx", format=source_ext.lstrip("."), outputfile=tmp_out_path)
            with open(tmp_out_path, "rb") as fh:
                return fh.read()
        else:
            result = pypandoc.convert_file(tmp_in.name, to=target, format=source_ext.lstrip("."))
            return result.encode("utf-8")
    except (OSError, RuntimeError):
        return None
    finally:
        try:
            os.remove(tmp_in.name)
        except OSError:
            pass

def _convert_doc_via_win32(data_bytes: bytes) -> bytes | None:
    """åœ¨Windowsç¯å¢ƒä¸‹å°†DOCè½¬ä¸ºDOCX"""
    if not HAS_WIN32_COM:
        return None
    temp_dir = tempfile.mkdtemp()
    doc_path = os.path.join(temp_dir, "legacy.doc")
    docx_path = os.path.join(temp_dir, "legacy.docx")
    with open(doc_path, "wb") as fh:
        fh.write(data_bytes)
    converted = None
    word = None
    doc_obj = None
    initialized = False
    try:
        pythoncom.CoInitialize()
        initialized = True
        word = win32com.client.Dispatch("Word.Application")
        word.Visible = False
        doc_obj = word.Documents.Open(doc_path)
        doc_obj.SaveAs(docx_path, FileFormat=16)
        doc_obj.Close(False)
        with open(docx_path, "rb") as fh:
            converted = fh.read()
    except Exception:
        converted = None
    finally:
        if doc_obj is not None:
            try:
                doc_obj.Close(False)
            except Exception:
                pass
        if word is not None:
            try:
                word.Quit()
            except Exception:
                pass
        if initialized:
            try:
                pythoncom.CoUninitialize()
            except Exception:
                pass
        for path in (doc_path, docx_path):
            if os.path.exists(path):
                try:
                    os.remove(path)
                except OSError:
                    pass
        try:
            os.rmdir(temp_dir)
        except OSError:
            pass
    return converted

# ==================== å½’æ¡£é¡µé¢ ====================
def render_archive_page():
    """æ¸²æŸ“ç§‘ç ”å½’æ¡£é¡µé¢"""
    st.markdown(f"""
    <div style="background: white; padding: 24px; border-radius: 16px; box-shadow: 0 4px 12px rgba(0,0,0,0.05);">
        <h1 style="color: {COLORS['primary']}; margin: 0 0 16px 0;">ğŸ“– ç§‘ç ”å½’æ¡£</h1>
        <p style="color: {COLORS['secondary']}; margin: 0;">ç®¡ç†å’Œå¯¼å‡ºæ‚¨çš„å®éªŒè®°å½•</p>
    </div>
    """, unsafe_allow_html=True)
    
    # æœç´¢å’Œç­›é€‰
    col_search, col_tag = st.columns([3, 1])
    search_term = col_search.text_input("ğŸ” æœç´¢", key="archive_search")
    
    tags_available = ["å…¨éƒ¨"] + get_distinct_tags()
    tag_choice = col_tag.selectbox("æ ‡ç­¾ç­›é€‰", tags_available, key="archive_tag")
    
    # ä¸€é”®è¿ç§»å†å²è®°å½•
    with st.expander("ğŸª„ ä¸€é”®è¿ç§»å†å²è®°å½•", expanded=False):
        st.caption("æ”¯æŒ Markdown / Word / TXT / CSV ç­‰æ ¼å¼ï¼Œä¿ç•™åŸå§‹è®°å½•å†…å®¹")
        
        legacy_files = st.file_uploader(
            "é€‰æ‹©æ—§å®éªŒè®°å½•æ–‡ä»¶",
            accept_multiple_files=True,
            type=["md", "markdown", "txt", "csv", "tsv", "doc", "docx", "rtf"],
            key="legacy_import_files"
        )
        
        col1, col2, col3 = st.columns(3)
        legacy_category = col1.selectbox("å¯¼å…¥ç±»åˆ«", ["ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"], key="legacy_category")
        legacy_tags = col2.text_input("ç»Ÿä¸€æ ‡ç­¾", "#å†å²è®°å½•", key="legacy_tags")
        legacy_date = col3.date_input("é»˜è®¤æ—¥æœŸ", datetime.now().date(), key="legacy_date")
        
        use_ai = st.checkbox("ä½¿ç”¨AIæå–å…ƒæ•°æ®ï¼ˆæ¨èï¼‰", value=True, key="use_ai_metadata")
        filename_date = st.checkbox("å°è¯•æ ¹æ®æ–‡ä»¶åæ¨æ–­æ—¥æœŸ", value=True, key="filename_date")
        
        if st.button("ğŸš€ å¼€å§‹è¿ç§»", type="primary", use_container_width=True):
            if not legacy_files:
                st.warning("è¯·å…ˆé€‰æ‹©è‡³å°‘ä¸€ä¸ªæ–‡ä»¶")
            else:
                with st.spinner("æ­£åœ¨è§£æå¹¶å¯¼å…¥å†å²è®°å½•..."):
                    import_results = import_legacy_records_preserve_original(
                        legacy_files,
                        default_category=legacy_category,
                        default_tags=legacy_tags,
                        default_date=legacy_date,
                        prefer_filename_date=filename_date,
                        use_ai_metadata=use_ai
                    )
                
                # æ˜¾ç¤ºç»“æœ
                success_items = [item for item in import_results if item.get("success")]
                failure_items = [item for item in import_results if not item.get("success")]
                
                if success_items:
                    st.success(f"âœ… æˆåŠŸå¯¼å…¥ {len(success_items)} æ¡è®°å½•")
                    for item in success_items:
                        with st.expander(f"âœ… {item['file']}"):
                            st.write(f"**ä»»åŠ¡å**: {item['task_name']}")
                            st.write(f"**æ—¥æœŸ**: {item['date']}")
                            st.write(f"**ç±»åˆ«**: {item['category']}")
                            st.write(f"**æ ‡ç­¾**: {item['tags']}")
                            st.write(f"**é¢„è§ˆ**: {item['content_preview']}")
                
                if failure_items:
                    st.error(f"âŒ {len(failure_items)} ä¸ªæ–‡ä»¶å¯¼å…¥å¤±è´¥")
                    for item in failure_items:
                        st.write(f"âš ï¸ {item['file']}: {item.get('message', 'æœªçŸ¥é”™è¯¯')}")
    
    # è‡ªåŠ¨ç”Ÿæˆå‘¨æŠ¥
    with st.expander("ğŸ—“ï¸ è‡ªåŠ¨ç”Ÿæˆå‘¨æŠ¥", expanded=False):
        reference_date = st.date_input("å‚è€ƒæ—¥æœŸ", datetime.now().date(), key="weekly_date")
        
        if st.button("ğŸ“„ ç”Ÿæˆå‘¨æŠ¥", key="weekly_btn"):
            start_date = (reference_date - timedelta(days=6)).strftime("%Y-%m-%d")
            end_date = reference_date.strftime("%Y-%m-%d")
            
            report_df = run_query(
                "SELECT date, task_name, details, tags FROM tasks WHERE category='ç§‘ç ”' AND details!='' AND date BETWEEN ? AND ? ORDER BY date",
                (start_date, end_date),
                fetch=True
            )
            
            if report_df.empty:
                st.warning("æ‰€é€‰æ—¶é—´æ®µå†…æš‚æ— å®éªŒè®°å½•")
            else:
                records = report_df.to_dict('records')
                client = get_ai_client()
                
                if client:
                    with st.spinner("AI æ­£åœ¨æ•´ç†å‘¨æŠ¥..."):
                        report_text = ai_generate_weekly_report(client, records, start_date, end_date)
                else:
                    report_text = build_weekly_report_fallback(records, start_date, end_date)
                
                st.text_area("å‘¨æŠ¥å†…å®¹", report_text, height=300)
                st.download_button(
                    "ğŸ“¥ å¯¼å‡ºå‘¨æŠ¥",
                    report_text.encode("utf-8"),
                    file_name=f"weekly_report_{end_date}.md",
                    mime="text/markdown",
                    use_container_width=True
                )
    
    # æŸ¥è¯¢è®°å½•
    base_sql = "SELECT * FROM tasks WHERE category='ç§‘ç ”' AND details!=''"
    params = []
    
    if search_term:
        wildcard = f"%{search_term}%"
        base_sql += " AND (task_name LIKE ? OR details LIKE ? OR tags LIKE ?)"
        params.extend([wildcard, wildcard, wildcard])
    
    if tag_choice and tag_choice != "å…¨éƒ¨":
        base_sql += " AND tags LIKE ?"
        params.append(f"%{tag_choice}%")
    
    df = run_query(base_sql + " ORDER BY date DESC", tuple(params), fetch=True)
    
    if df.empty:
        st.info("ğŸ“­ æš‚æ—¶æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å®éªŒè®°å½•")
    else:
        # æ‰¹é‡å¯¼å‡º
        st.markdown("### ğŸ“¤ æ‰¹é‡å¯¼å‡º")
        archive_exports = get_archive_exports(df.to_dict('records'))
        
        if archive_exports:
            exp_cols = st.columns(len(archive_exports))
            for col, item in zip(exp_cols, archive_exports):
                label, fname, data, mime = item
                with col:
                    st.download_button(
                        f"ğŸ“„ å¯¼å‡º{label}",
                        data,
                        file_name=fname,
                        mime=mime,
                        use_container_width=True
                    )
        
        st.divider()
        
        # è®°å½•åˆ—è¡¨
        st.markdown(f"### ğŸ“‹ å®éªŒè®°å½•åˆ—è¡¨ ({len(df)}æ¡)")
        
        for _, r in df.iterrows():
            with st.expander(f"**{r['date']}** | {r['task_name']}", expanded=False):
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.caption(f"ğŸ·ï¸ æ ‡ç­¾ï¼š{r['tags'] or '-'} Â· ğŸ“‚ ç±»å‹ï¼š{r['category']}")
                    st.markdown(r['details'])
                with col2:
                    if st.button("ğŸ“ ç¼–è¾‘", key=f"edit_{r['id']}", use_container_width=True):
                        show_record_editor_dialog(int(r['id']))
                    
                    # å•ä¸ªå¯¼å‡º
                    exports = get_record_exports(r)
                    for label, fname, data, mime in exports:
                        st.download_button(
                            f"ğŸ“„ {label}",
                            data,
                            file_name=fname,
                            mime=mime,
                            key=f"export_{r['id']}_{label}",
                            use_container_width=True
                        )

def render_analytics_page():
    """æ¸²æŸ“æ•°æ®åˆ†æé¡µé¢"""
    st.markdown(f"""
    <div style="background: white; padding: 24px; border-radius: 16px; box-shadow: 0 4px 12px rgba(0,0,0,0.05);">
        <h1 style="color: {COLORS['primary']}; margin: 0 0 16px 0;">ğŸ“Š æ•°æ®åˆ†æ</h1>
        <p style="color: {COLORS['secondary']}; margin: 0;">å¯è§†åŒ–æ‚¨çš„ç§‘ç ”å·¥ä½œæ•°æ®</p>
    </div>
    """, unsafe_allow_html=True)
    
    # è·å–æ•°æ®
    df = run_query("SELECT * FROM tasks ORDER BY date", fetch=True)
    
    if df.empty:
        st.info("ğŸ“­ æš‚æ— æ•°æ®å¯ä¾›åˆ†æ")
        return
    
    # ç»Ÿè®¡å¡ç‰‡
    st.markdown("### ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
    col1, col2, col3, col4 = st.columns(4)
    
    total_tasks = len(df)
    completed_tasks = len(df[df['is_done'] == 1])
    research_tasks = len(df[df['category'] == 'ç§‘ç ”'])
    this_week_tasks = len(df[df['date'] >= (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')])
    
    with col1:
        st.metric("æ€»ä»»åŠ¡æ•°", total_tasks)
    with col2:
        st.metric("å·²å®Œæˆ", completed_tasks)
    with col3:
        st.metric("ç§‘ç ”ä»»åŠ¡", research_tasks)
    with col4:
        st.metric("æœ¬å‘¨ä»»åŠ¡", this_week_tasks)
    
    # å›¾è¡¨åŒºåŸŸ
    col_chart1, col_chart2 = st.columns(2)
    
    with col_chart1:
        st.markdown("### ğŸ“… å·¥ä½œé‡è¶‹åŠ¿")
        workload_chart = create_workload_chart(df)
        if workload_chart:
            st.plotly_chart(workload_chart, use_container_width=True)
    
    with col_chart2:
        st.markdown("### ğŸ¥§ ç±»åˆ«åˆ†å¸ƒ")
        category_chart = create_category_pie_chart(df)
        if category_chart:
            st.plotly_chart(category_chart, use_container_width=True)
    
    # æ ‡ç­¾äº‘
    st.markdown("### ğŸ·ï¸ æ ‡ç­¾åˆ†æ")
    all_tags = []
    for tags in df['tags'].dropna():
        tag_list = re.split(r'[,ï¼Œ\s]+', str(tags))
        all_tags.extend([tag.strip() for tag in tag_list if tag.strip() and tag.strip().startswith('#')])
    
    if all_tags:
        tag_counts = Counter(all_tags)
        top_tags = dict(tag_counts.most_common(20))
        
        # åˆ›å»ºæ ‡ç­¾äº‘å¯è§†åŒ–
        tag_df = pd.DataFrame(list(top_tags.items()), columns=['æ ‡ç­¾', 'æ¬¡æ•°'])
        fig = px.bar(tag_df, x='æ¬¡æ•°', y='æ ‡ç­¾', orientation='h', 
                     title='å¸¸ç”¨æ ‡ç­¾ç»Ÿè®¡ (Top 20)')
        fig.update_layout(
            font=dict(family=FONTS['family'], size=12),
            plot_bgcolor='white',
            paper_bgcolor='white'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # å¯¼å‡ºç»Ÿè®¡æ•°æ®
    st.markdown("### ğŸ“¤ å¯¼å‡ºæ•°æ®")
    if st.button("ğŸ“Š å¯¼å‡ºCSVæŠ¥å‘Š", use_container_width=True):
        csv_data = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            "ğŸ’¾ ä¸‹è½½CSV",
            csv_data,
            file_name=f"lab_report_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            use_container_width=True
        )

# ==================== å¯¼å‡ºåŠŸèƒ½ï¼ˆç»§ç»­ï¼‰ ====================
def build_archive_markdown(rows):
    """æ„å»ºå½’æ¡£Markdown"""
    sections = [build_record_markdown(r) for r in rows]
    return "\n\n---\n\n".join(sections)

def build_archive_docx_bytes(rows):
    """æ„å»ºå½’æ¡£DOCX"""
    doc = Document()
    for idx, row in enumerate(rows):
        if idx > 0:
            doc.add_page_break()
        row = normalize_task_row(row)
        doc.add_heading(row.get("task_name", "å®éªŒè®°å½•"), level=1)
        doc.add_paragraph(f"æ—¥æœŸï¼š{row.get('date', '-')}")
        doc.add_paragraph(f"ç±»å‹ï¼š{row.get('category', '-')}")
        doc.add_paragraph(f"æ ‡ç­¾ï¼š{row.get('tags') or '-'}")
        doc.add_heading("å®éªŒè®°å½•", level=2)
        doc.add_paragraph(row.get("details") or "(æš‚æ— å®éªŒè®°å½•)")
    bio = BytesIO()
    doc.save(bio)
    return bio.getvalue()

def get_archive_exports(rows):
    """è·å–å½’æ¡£å¯¼å‡ºæ•°æ®"""
    if not rows:
        return []
    rows = [normalize_task_row(r) for r in rows]
    timestamp = datetime.now().strftime("%Y%m%d")
    base = f"lab_archive_{timestamp}"
    md_bytes = build_archive_markdown(rows).encode("utf-8")
    docx_bytes = build_archive_docx_bytes(rows)
    return [
        ("MD", f"{base}.md", md_bytes, "text/markdown"),
        ("DOCX", f"{base}.docx", docx_bytes, "application/vnd.openxmlformats-officedocument.wordprocessingml.document"),
    ]

def build_weekly_report_fallback(records, start_date, end_date):
    """æ— AIæ—¶çš„ç®€å•å‘¨æŠ¥æ‹¼æ¥"""
    lines = [f"# å‘¨æŠ¥ï¼ˆ{start_date} ~ {end_date}ï¼‰", ""]
    for row in records:
        snippet = (row.get("details") or "").replace("\n", " ")
        snippet = re.sub(r"\s+", " ", snippet)
        if len(snippet) > 160:
            snippet = snippet[:160] + "â€¦"
        lines.append(f"- {row.get('date', '')} {row.get('task_name', '')}ï¼š{snippet}")
    return "\n".join(lines)
