import streamlit as st
import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from streamlit_calendar import calendar
import os
import shutil
import time
import json
import re
import uuid
import wave
import struct
import gzip
import hashlib
import secrets as py_secrets
import hmac
import smtplib
import ssl
import zipfile
import base64
import tempfile
from io import BytesIO
from email.message import EmailMessage
from docx import Document
from docx.oxml.table import CT_Tbl
from docx.oxml.text.paragraph import CT_P
from docx.table import Table, _Cell
from docx.text.paragraph import Paragraph
from openai import OpenAI

try:
    import audioop  # removed in newer Python versions; optional in this app
except Exception:
    audioop = None

def _load_dotenv(dotenv_path: str = ".env") -> None:
    """Load simple KEY=VALUE pairs from .env into os.environ (no external deps)."""
    if not os.path.exists(dotenv_path):
        return
    try:
        with open(dotenv_path, "r", encoding="utf-8") as fh:
            for raw_line in fh:
                line = raw_line.strip()
                if not line or line.startswith("#"):
                    continue
                if "=" not in line:
                    continue
                key, value = line.split("=", 1)
                key = key.strip()
                value = value.strip().strip("'").strip('"')
                if not key:
                    continue
                os.environ.setdefault(key, value)
    except Exception:
        return


def _get_setting(name: str, default: str = "") -> str:
    """Read settings from Streamlit secrets first, then env vars."""
    try:
        if hasattr(st, "secrets") and name in st.secrets:
            return str(st.secrets[name])
    except Exception:
        pass
    return os.getenv(name, default)


_load_dotenv()

try:
    import pypandoc
except Exception:
    pypandoc = None

try:
    import win32com.client
    import pythoncom
except Exception:
    win32com = None
    pythoncom = None

HAS_PYPANDOC = pypandoc is not None
HAS_WIN32_COM = win32com is not None

# --- é…ç½®åŒº ---
LEGACY_UPLOAD_DIR = "uploads"
LEGACY_BACKUP_DIR = "backups"
LEGACY_DB_PATH = "my_lab_data.db"
DATA_DIR = "data"


def _get_streamlit_user_email() -> str | None:
    """Best-effort: return the email of the signed-in Streamlit user (Cloud/private apps)."""
    try:
        if hasattr(st, "user"):
            info = st.user.to_dict() or {}
            email = info.get("email")
            if email:
                return str(email).strip().lower()
    except Exception:
        pass
    return None


def _parse_csv_list(value: str) -> list[str]:
    if not value:
        return []
    parts = re.split(r"[,\s]+", str(value))
    return [p.strip().lower() for p in parts if p.strip()]


def _get_auth_settings() -> dict:
    mode = str(_get_setting("LAB_DIARY_AUTH_MODE", "")).strip().lower()  # "email_otp" | "password" | ""
    allowed_domains = _parse_csv_list(_get_setting("LAB_DIARY_ALLOWED_EMAIL_DOMAINS", ""))
    allowed_emails = _parse_csv_list(_get_setting("LAB_DIARY_ALLOWED_EMAILS", ""))
    session_minutes = int(str(_get_setting("LAB_DIARY_AUTH_SESSION_MINUTES", "720")).strip() or "720")
    code_minutes = int(str(_get_setting("LAB_DIARY_AUTH_CODE_MINUTES", "10")).strip() or "10")
    dev_show_code = str(_get_setting("LAB_DIARY_AUTH_DEV_SHOW_CODE", "0")).strip().lower() in ("1", "true", "yes")
    shared_password = str(_get_setting("LAB_DIARY_SHARED_PASSWORD", "")).strip()
    shared_password_hash = str(_get_setting("LAB_DIARY_SHARED_PASSWORD_HASH", "")).strip().lower()

    smtp_host = str(_get_setting("SMTP_HOST", "")).strip()
    smtp_port = int(str(_get_setting("SMTP_PORT", "587")).strip() or "587")
    smtp_user = str(_get_setting("SMTP_USER", "")).strip()
    smtp_password = str(_get_setting("SMTP_PASSWORD", "")).strip()
    smtp_from = str(_get_setting("SMTP_FROM", "")).strip()
    smtp_use_tls = str(_get_setting("SMTP_USE_TLS", "1")).strip().lower() in ("1", "true", "yes")
    smtp_use_ssl = str(_get_setting("SMTP_USE_SSL", "0")).strip().lower() in ("1", "true", "yes")
    smtp_timeout = int(str(_get_setting("SMTP_TIMEOUT", "20")).strip() or "20")
    smtp_debug = str(_get_setting("SMTP_DEBUG", "0")).strip().lower() in ("1", "true", "yes")

    return {
        "mode": mode,
        "allowed_domains": allowed_domains,
        "allowed_emails": allowed_emails,
        "session_minutes": session_minutes,
        "code_minutes": code_minutes,
        "dev_show_code": dev_show_code,
        "shared_password": shared_password,
        "shared_password_hash": shared_password_hash,
        "smtp_host": smtp_host,
        "smtp_port": smtp_port,
        "smtp_user": smtp_user,
        "smtp_password": smtp_password,
        "smtp_from": smtp_from,
        "smtp_use_tls": smtp_use_tls,
        "smtp_use_ssl": smtp_use_ssl,
        "smtp_timeout": smtp_timeout,
        "smtp_debug": smtp_debug,
    }


def _is_allowed_login_email(email: str, auth: dict) -> bool:
    email = (email or "").strip().lower()
    if not email or "@" not in email:
        return False
    if auth["allowed_emails"] and email not in auth["allowed_emails"]:
        return False
    if auth["allowed_domains"]:
        domain = email.split("@", 1)[1]
        if domain not in auth["allowed_domains"]:
            return False
    return True


def _send_email_login_code(email: str, code: str, auth: dict) -> None:
    if auth["dev_show_code"]:
        return
    if not auth["smtp_host"] or not auth["smtp_from"]:
        raise RuntimeError("Missing SMTP configuration (SMTP_HOST/SMTP_FROM).")

    msg = EmailMessage()
    msg["Subject"] = "Lab Diary AI ç™»å½•éªŒè¯ç "
    msg["From"] = auth["smtp_from"]
    msg["To"] = email
    msg.set_content(
        f"ä½ çš„ç™»å½•éªŒè¯ç æ˜¯ï¼š{code}\n\n"
        f"æœ‰æ•ˆæœŸï¼š{auth['code_minutes']} åˆ†é’Ÿ\n\n"
        "å¦‚éæœ¬äººæ“ä½œï¼Œè¯·å¿½ç•¥æ­¤é‚®ä»¶ã€‚"
    )

    host = auth["smtp_host"]
    port = auth["smtp_port"]
    timeout = auth["smtp_timeout"]
    context = ssl.create_default_context()

    step = "connect"
    try:
        if auth["smtp_use_ssl"]:
            server = smtplib.SMTP_SSL(host, port, timeout=timeout, context=context)
        else:
            server = smtplib.SMTP(host, port, timeout=timeout)
        try:
            if auth["smtp_debug"]:
                server.set_debuglevel(1)
            step = "ehlo"
            server.ehlo()
            if auth["smtp_use_tls"] and not auth["smtp_use_ssl"]:
                step = "starttls"
                server.starttls(context=context)
                step = "ehlo_after_starttls"
                server.ehlo()
            if auth["smtp_user"] and auth["smtp_password"]:
                step = "login"
                server.login(auth["smtp_user"], auth["smtp_password"])
            step = "send_message"
            server.send_message(msg)
        finally:
            try:
                step = "quit"
                server.quit()
            except Exception:
                pass
    except Exception as exc:
        raise RuntimeError(
            f"SMTP failed at step={step}. "
            f"Check SMTP_HOST/PORT, TLS vs SSL (587=STARTTLS, 465=SSL), and whether the hosting platform blocks outbound SMTP. "
            f"Original error: {exc!r}"
        ) from exc


def _clear_auth_session() -> None:
    for key in list(st.session_state.keys()):
        if key.startswith("auth_"):
            del st.session_state[key]


def require_app_login() -> None:
    """
    In-app login page (email OTP).
    Enable by setting `LAB_DIARY_AUTH_MODE=email_otp` in Streamlit Cloud secrets.
    """
    auth = _get_auth_settings()
    if auth["mode"] not in ("email_otp", "password"):
        return

    now = datetime.now()
    authed_email = (st.session_state.get("auth_email") or "").strip().lower()
    expires_at = st.session_state.get("auth_expires_at")
    if authed_email and isinstance(expires_at, datetime) and expires_at > now:
        return

    st.set_page_config(page_title="Lab Diary AI ç™»å½•", layout="centered", page_icon="ğŸ”")
    st.title("ğŸ” ç™»å½• Lab Diary AI")
    if auth["mode"] == "email_otp":
        st.caption("è¾“å…¥é‚®ç®±è·å–éªŒè¯ç ç™»å½•ã€‚")
    else:
        st.caption("è¾“å…¥é‚®ç®± + å…±äº«å£ä»¤ç™»å½•ï¼ˆæ— éœ€å‘éªŒè¯ç ï¼‰ã€‚")

    email = st.text_input("é‚®ç®±", key="auth_input_email", placeholder="name@domain.com").strip().lower()
    if email and not _is_allowed_login_email(email, auth):
        if auth["allowed_domains"] or auth["allowed_emails"]:
            st.error("è¯¥é‚®ç®±ä¸åœ¨å…è®¸èŒƒå›´å†…ã€‚")
        else:
            st.warning("å½“å‰æœªé…ç½®å…è®¸é‚®ç®±/åŸŸåç™½åå•ï¼šä»»ä½•é‚®ç®±éƒ½å¯ä»¥è¯·æ±‚éªŒè¯ç ã€‚å»ºè®®è®¾ç½® `LAB_DIARY_ALLOWED_EMAIL_DOMAINS`ã€‚")

    if auth["mode"] == "email_otp":
        col1, col2 = st.columns(2)
        with col1:
            if st.button("å‘é€éªŒè¯ç ", use_container_width=True):
                if not email or "@" not in email:
                    st.error("è¯·è¾“å…¥æ­£ç¡®çš„é‚®ç®±ã€‚")
                elif not _is_allowed_login_email(email, auth):
                    st.error("è¯¥é‚®ç®±ä¸åœ¨å…è®¸èŒƒå›´å†…ã€‚")
                else:
                    code = f"{py_secrets.randbelow(1000000):06d}"
                    st.session_state["auth_pending_email"] = email
                    st.session_state["auth_code_hash"] = hashlib.sha256(code.encode("utf-8")).hexdigest()
                    st.session_state["auth_code_expires_at"] = now + timedelta(minutes=auth["code_minutes"])
                    st.session_state["auth_code_sent_at"] = now
                    try:
                        _send_email_login_code(email, code, auth)
                        if auth["dev_show_code"]:
                            st.info(f"DEV æ¨¡å¼ï¼šéªŒè¯ç æ˜¯ {code}")
                        st.success("éªŒè¯ç å·²å‘é€ï¼Œè¯·æŸ¥æ”¶é‚®ç®±ã€‚")
                    except Exception as exc:
                        st.error(f"å‘é€å¤±è´¥ï¼š{exc}")

        with col2:
            code_input = st.text_input("éªŒè¯ç ", key="auth_input_code", placeholder="6 ä½æ•°å­—").strip()
            if st.button("ç™»å½•", type="primary", use_container_width=True):
                pending_email = (st.session_state.get("auth_pending_email") or "").strip().lower()
                code_hash = st.session_state.get("auth_code_hash")
                code_expires = st.session_state.get("auth_code_expires_at")

                if not pending_email:
                    st.error("è¯·å…ˆå‘é€éªŒè¯ç ã€‚")
                elif not isinstance(code_expires, datetime) or code_expires <= now:
                    st.error("éªŒè¯ç å·²è¿‡æœŸï¼Œè¯·é‡æ–°å‘é€ã€‚")
                elif not code_input or not code_input.isdigit() or len(code_input) != 6:
                    st.error("è¯·è¾“å…¥ 6 ä½æ•°å­—éªŒè¯ç ã€‚")
                else:
                    input_hash = hashlib.sha256(code_input.encode("utf-8")).hexdigest()
                    if input_hash != code_hash:
                        st.error("éªŒè¯ç é”™è¯¯ã€‚")
                    else:
                        st.session_state["auth_email"] = pending_email
                        st.session_state["auth_expires_at"] = now + timedelta(minutes=auth["session_minutes"])
                        for k in ("auth_pending_email", "auth_code_hash", "auth_code_expires_at", "auth_code_sent_at", "auth_input_code"):
                            if k in st.session_state:
                                del st.session_state[k]
                        st.success("ç™»å½•æˆåŠŸã€‚")
                        st.rerun()
    else:
        password_input = st.text_input("å…±äº«å£ä»¤", key="auth_input_password", type="password").strip()
        if st.button("ç™»å½•", type="primary", use_container_width=True):
            if not email or "@" not in email:
                st.error("è¯·è¾“å…¥æ­£ç¡®çš„é‚®ç®±ã€‚")
            elif not _is_allowed_login_email(email, auth):
                st.error("è¯¥é‚®ç®±ä¸åœ¨å…è®¸èŒƒå›´å†…ã€‚")
            elif not password_input:
                st.error("è¯·è¾“å…¥å…±äº«å£ä»¤ã€‚")
            elif not auth.get("shared_password") and not auth.get("shared_password_hash"):
                st.error("æœªé…ç½®å…±äº«å£ä»¤ï¼šè¯·åœ¨ Secrets ä¸­è®¾ç½® `LAB_DIARY_SHARED_PASSWORD` æˆ– `LAB_DIARY_SHARED_PASSWORD_HASH`ã€‚")
            else:
                ok = False
                if auth.get("shared_password_hash"):
                    digest = hashlib.sha256(password_input.encode("utf-8")).hexdigest().lower()
                    ok = hmac.compare_digest(digest, auth["shared_password_hash"])
                elif auth.get("shared_password"):
                    ok = hmac.compare_digest(password_input, auth["shared_password"])
                if not ok:
                    st.error("å…±äº«å£ä»¤é”™è¯¯ã€‚")
                else:
                    st.session_state["auth_email"] = email
                    st.session_state["auth_expires_at"] = now + timedelta(minutes=auth["session_minutes"])
                    for k in ("auth_pending_email", "auth_code_hash", "auth_code_expires_at", "auth_code_sent_at", "auth_input_code", "auth_input_password"):
                        if k in st.session_state:
                            del st.session_state[k]
                    st.success("ç™»å½•æˆåŠŸã€‚")
                    st.rerun()

    st.divider()
    if auth["mode"] == "email_otp":
        st.caption("éœ€è¦å¸®åŠ©ï¼Ÿè¯·è”ç³»ç®¡ç†å‘˜é…ç½®é‚®ä»¶æœåŠ¡å™¨ï¼ˆSMTPï¼‰å’Œå…è®¸åŸŸåç™½åå•ã€‚")
    else:
        st.caption("éœ€è¦å¸®åŠ©ï¼Ÿè¯·è”ç³»ç®¡ç†å‘˜é…ç½®å…±äº«å£ä»¤å’Œå…è®¸åŸŸåç™½åå•ã€‚")
    st.stop()


def get_storage_paths() -> dict:
    """
    Multi-user isolation:
    - If Streamlit provides a signed-in user email, store data in `data/users/<hash>/`.
    - Otherwise (local/dev), fall back to legacy paths in the repo root.
    """
    session_email = str(st.session_state.get("auth_email", "")).strip().lower()
    override_email = _get_setting("LAB_DIARY_USER_EMAIL", "").strip().lower()
    email = session_email or override_email or _get_streamlit_user_email()
    if email:
        digest = hashlib.sha256(email.encode("utf-8")).hexdigest()[:16]
        root = os.path.join(DATA_DIR, "users", digest)
        upload_dir = os.path.join(root, "uploads")
        backup_dir = os.path.join(root, "backups")
        db_path = os.path.join(root, "my_lab_data.db")
        user_label = email
    else:
        root = "."
        upload_dir = LEGACY_UPLOAD_DIR
        backup_dir = LEGACY_BACKUP_DIR
        db_path = LEGACY_DB_PATH
        user_label = "local"
    os.makedirs(upload_dir, exist_ok=True)
    os.makedirs(backup_dir, exist_ok=True)
    return {
        "user_label": user_label,
        "root": root,
        "upload_dir": upload_dir,
        "backup_dir": backup_dir,
        "db_path": db_path,
    }

# --- è¯­éŸ³è¯†åˆ«ï¼ˆæš‚æ—¶ä¸‹çº¿ï¼‰---
# ä½ ä¹‹å‰é…ç½®çš„ç«å±±å¼•æ“è¯­éŸ³è¯†åˆ«ç›¸å…³ä»£ç å·²å•ç‹¬å­˜æ¡£ï¼Œæ–¹ä¾¿ä¹‹åæ¢å¤ï¼š
# è§ `archived/volc_asr_reference.py`

# --- DeepSeek AI é…ç½® ---
DEEPSEEK_API_KEY = _get_setting("DEEPSEEK_API_KEY", "")
DEEPSEEK_BASE_URL = _get_setting("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
DEEPSEEK_MODEL = _get_setting("DEEPSEEK_MODEL", "deepseek-chat")

# --- è®¾è®¡é£æ ¼é…ç½® ---
# è‰²å½©ç³»ç»Ÿ
COLORS = {
    'primary': '#1e293b',      # æ·±è“ç° - ä¸»è‰²
    'secondary': '#334155',    # ä¸­è“ç° - è¾…åŠ©è‰²
    'background': '#f8fafc',   # ææµ…ç° - èƒŒæ™¯
    'accent': '#3b82f6',       # ç§‘æŠ€è“ - å¼ºè°ƒ
    'success': '#10b981',      # æˆåŠŸç»¿
    'warning': '#f59e0b',      # è­¦å‘Šæ©™
    'error': '#ef4444',        # é”™è¯¯çº¢
    'info': '#06b6d4',         # ä¿¡æ¯è“
    'research': '#8b5cf6',     # ç§‘ç ”ç´«
    'clinical': '#06b6d4',     # ä¸´åºŠé’
    'course': '#f59e0b',       # è¯¾ç¨‹é‡‘
    'other': '#6b7280',        # ä¸­æ€§ç°
}

# å­—ä½“
FONTS = {
    'family': 'Inter, -apple-system, BlinkMacSystemFont, sans-serif',
    'mono': 'JetBrains Mono, monospace'
}

for d in [LEGACY_UPLOAD_DIR, LEGACY_BACKUP_DIR]:
    os.makedirs(d, exist_ok=True)

# ==================== å·¥å…·å‡½æ•° ====================
def get_versioned_upload_path(filename):
    """Return upload path plus versioned filename to avoid overwriting."""
    upload_dir = get_storage_paths()["upload_dir"]
    base, ext = os.path.splitext(filename)
    candidate = filename
    idx = 1
    while os.path.exists(os.path.join(upload_dir, candidate)):
        candidate = f"{base}_v{idx}{ext}"
        idx += 1
    return os.path.join(upload_dir, candidate), candidate

def normalize_task_row(row):
    if isinstance(row, dict):
        return row
    if hasattr(row, "to_dict"):
        return row.to_dict()
    return row

def sanitize_filename(value, default="record"):
    value = re.sub(r"[^\w\-]+", "_", value).strip("_")
    return value or default

def shorten_task_name(name: str, max_length: int = 28) -> str:
    """å¼ºåˆ¶ä»»åŠ¡åç®€æ´ä¸”å»é™¤å†—ä½™æ ‡ç‚¹"""
    if not name:
        return "æœªå‘½åä»»åŠ¡"
    clean = re.sub(r"\s+", " ", str(name)).strip()
    clean = re.sub(r"[ï¼Œã€‚,.ï¼›;ã€ï¼š:]+$", "", clean)
    if len(clean) > max_length:
        clean = clean[:max_length].rstrip() + "â€¦"
    return clean

# ==================== AI åŠŸèƒ½ ====================
def get_ai_client():
    """è·å– DeepSeek AI å®¢æˆ·ç«¯"""
    if not DEEPSEEK_API_KEY:
        return None
    return OpenAI(base_url=DEEPSEEK_BASE_URL, api_key=DEEPSEEK_API_KEY)

def ai_polish_text(client, text, extra_instruction=None):
    """AI æ¶¦è‰²åŠŸèƒ½"""
    if not text:
        return "è¯·å…ˆè¾“å…¥æ–‡æœ¬ã€‚"
    user_content = text
    if extra_instruction:
        user_content = f"{text}\n\n[è¡¥å……è¦æ±‚]\n{extra_instruction}"
    try:
        response = client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç¥ç»ç§‘å­¦åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„å®éªŒè®°å½•æ¶¦è‰²ä¸ºå­¦æœ¯é£æ ¼ã€‚ç›´æ¥è¾“å‡ºç»“æœã€‚"},
                {"role": "user", "content": user_content}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {e}"

def ai_extract_metadata(client, text):
    """
    AIåªæå–å…ƒæ•°æ®ï¼Œä¸ä¿®æ”¹åŸå§‹å†…å®¹
    è¿”å›: {date, task_name, category, tags}
    """
    try:
        response = client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªå…ƒæ•°æ®æå–åŠ©æ‰‹ã€‚ä»å®éªŒè®°å½•ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œä½†ä¸è¦ä¿®æ”¹åŸå§‹è®°å½•å†…å®¹ï¼š
1. æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
2. ä»»åŠ¡/å®éªŒåç§°
3. ç±»åˆ«ï¼ˆç§‘ç ”/ä¸´åºŠ/è¯¾ç¨‹/å…¶ä»–ï¼‰
4. æ ‡ç­¾ï¼ˆä»¥#å¼€å¤´ï¼Œå¤šä¸ªæ ‡ç­¾ç”¨ç©ºæ ¼åˆ†éš”ï¼‰

åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚"""},
                {"role": "user", "content": text[:2000]}  # é™åˆ¶è¾“å…¥é•¿åº¦
            ],
            response_format={"type": "json_object"}
        )
        content = response.choices[0].message.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        return json.loads(content)
    except Exception as e:
        print(f"AI metadata extraction error: {e}")
        return {}

def ai_parse_schedule(client, text, attachment_notes=None):
    """å°†å¤§ç™½è¯è½¬æ¢ä¸ºç»“æ„åŒ–çš„ JSON ä»»åŠ¡åˆ—è¡¨"""
    today_str = datetime.now().strftime("%Y-%m-%d")
    weekday_str = datetime.now().strftime("%A")
    
    system_prompt = f"""
    You are a smart scheduler. Today is {today_str} ({weekday_str}).
    Extract tasks from the user's natural language description.
    
    Rules:
    1. Calculate exact dates (e.g., "next Friday", "tomorrow", "for 3 days").
    2. Return a JSON object containing a list under key "tasks".
    3. Each task must have: "date" (YYYY-MM-DD), "task_name", "category" (choose from: ç§‘ç ”, ä¸´åºŠ, è¯¾ç¨‹, å…¶ä»–), "tags" (string starting with #).
    4. Include "record_outline": 1-3 sentences summarizing what should be recorded in the experiment log (å…³é”®ææ–™/æ“ä½œ/è§‚å¯Ÿ) for this task.
    5. Do not output markdown code blocks, just raw JSON.
    """
    
    try:
        response = client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"{text}\n\n[é™„ä»¶å‚è€ƒ]\n{attachment_notes}" if attachment_notes else text}
            ],
            response_format={"type": "json_object"}
        )
        content = response.choices[0].message.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        data = json.loads(content)
        return data.get("tasks", [])
    except Exception as e:
        print(f"AI Parse Error: {e}")
        return []

def ai_generate_weekly_report(client, records, start_date, end_date):
    """åŸºäºè¿‘ 7 å¤©çš„è®°å½•è‡ªåŠ¨ç”Ÿæˆå‘¨æŠ¥å†…å®¹"""
    if not records:
        return ""
    bullet_lines = []
    for row in records:
        snippet = (row.get("details") or "").replace("\n", " ")
        snippet = re.sub(r"\s+", " ", snippet)
        if len(snippet) > 200:
            snippet = snippet[:200] + "â€¦"
        bullet_lines.append(f"- {row.get('date', '')} {row.get('task_name', '')}ï¼š{snippet}")
    prompt = f"""ä½ æ˜¯ç§‘ç ”åŠ©ç†ï¼Œè¯·å°†ä»¥ä¸‹ {len(records)} æ¡å®éªŒè®°å½•æ•´ç†ä¸ºä¸€ç¯‡ç»“æ„åŒ–çš„ç§‘ç ”å‘¨æŠ¥ï¼Œçªå‡ºå…³é”®è¿›å±•ã€é—®é¢˜ä¸ä¸‹ä¸€æ­¥è®¡åˆ’ã€‚
æ—¶é—´åŒºé—´ï¼š{start_date} ~ {end_date}

{chr(10).join(bullet_lines)}
"""
    try:
        resp = client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ç»éªŒä¸°å¯Œçš„å®éªŒå®¤PIï¼Œæ“…é•¿å°†è®°å½•æ•´ç†æˆå‘¨æŠ¥ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸šã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        return resp.choices[0].message.content
    except Exception as exc:
        return f"ç”Ÿæˆå¤±è´¥ï¼š{exc}"

# ==================== æ–‡æ¡£å¤„ç† ====================
FONT_CANDIDATES = [
    r"C:\Windows\Fonts\msyh.ttc",
    r"C:\Windows\Fonts\msyh.ttf",
    r"C:\Windows\Fonts\simhei.ttf",
    r"C:\Windows\Fonts\simfang.ttf",
]
TEXT_LIKE_EXTS = {".txt", ".md", ".markdown", ".csv", ".tsv", ".json", ".yaml", ".yml", ".log"}
LEGACY_TEXT_EXTS = {".md", ".markdown", ".txt", ".docx", ".doc", ".rtf"}
LEGACY_IMAGE_EXTS = {".png", ".jpg", ".jpeg", ".gif", ".bmp", ".tif", ".tiff", ".svg"}
IMAGE_MIME_MAP = {
    ".png": "image/png",
    ".jpg": "image/jpeg",
    ".jpeg": "image/jpeg",
    ".gif": "image/gif",
    ".bmp": "image/bmp",
    ".tif": "image/tiff",
    ".tiff": "image/tiff",
    ".svg": "image/svg+xml",
}

def _decode_text_preview(data: bytes, max_chars: int = 1500) -> str:
    """å°è¯•å¤šç§ç¼–ç è·å–æ–‡æœ¬ç‰‡æ®µ"""
    for enc in ("utf-8", "gbk", "latin-1"):
        try:
            text = data.decode(enc)
            break
        except UnicodeDecodeError:
            continue
    else:
        text = data.decode("utf-8", errors="ignore")
    text = text.strip()
    return (text[:max_chars] + "â€¦") if len(text) > max_chars else text

def _decode_text_full(data: bytes, strip: bool = True) -> str:
    """å°è¯•å¤šç§ç¼–ç è§£ç ä¸ºå®Œæ•´å­—ç¬¦ä¸²"""
    for enc in ("utf-8", "utf-16", "gbk", "latin-1"):
        try:
            text = data.decode(enc)
            break
        except UnicodeDecodeError:
            continue
    else:
        text = data.decode("utf-8", errors="ignore")
    return text.strip() if strip else text

def _persist_image_as_markdown(data: bytes, original_name: str) -> str:
    """ä¿å­˜å›¾ç‰‡åˆ° uploadsï¼Œå¹¶è¿”å›å†…è” Markdown"""
    ext = os.path.splitext(original_name)[1].lower()
    if ext not in IMAGE_MIME_MAP:
        ext = ".png"
    base = sanitize_filename(os.path.splitext(original_name)[0] or "legacy_image")
    filename = f"{base}{ext}"
    save_path, stored_name = get_versioned_upload_path(filename)
    with open(save_path, "wb") as fh:
        fh.write(data)
    mime = IMAGE_MIME_MAP.get(ext, "application/octet-stream")
    payload = base64.b64encode(data).decode("ascii")
    data_uri = f"data:{mime};base64,{payload}"
    note_path = save_path.replace("\\", "/")
    return f"![{stored_name}]({data_uri})\n\n_åŸå›¾å·²ä¿å­˜ï¼š{note_path}_"

def docx_to_markdown_with_assets(docx_bytes: bytes, origin_name: str) -> str:
    """å°† DOCX è½¬ Markdownï¼Œä¿ç•™æ®µè½ã€è¡¨æ ¼ã€å›¾ç‰‡"""
    doc = Document(BytesIO(docx_bytes))
    lines = []
    for block in _iter_docx_block_items(doc):
        if isinstance(block, Paragraph):
            chunk = _docx_paragraph_to_markdown(block)
            if chunk:
                lines.append(chunk)
        elif isinstance(block, Table):
            table_md = _docx_table_to_markdown(block)
            if table_md:
                lines.append(table_md)
    image_lines = _collect_docx_image_markdown(docx_bytes, origin_name)
    if image_lines:
        lines.append("### é™„ä»¶å›¾ç‰‡")
        lines.extend(image_lines)
    return "\n\n".join(lines).strip()

def convert_document_bytes_to_markdown(data_bytes: bytes, origin_name: str, ext: str) -> str:
    """ç»Ÿä¸€å…¥å£ï¼šå°† doc/docx/rtf è½¬ä¸º Markdown"""
    ext = ext.lower()
    if ext == ".docx":
        return docx_to_markdown_with_assets(data_bytes, origin_name)
    if ext == ".doc":
        converted = _pandoc_convert(data_bytes, ".doc", "docx")
        if not converted:
            converted = _convert_doc_via_win32(data_bytes)
        if converted:
            return docx_to_markdown_with_assets(converted, origin_name)
        fallback = _pandoc_convert(data_bytes, ".doc", "md")
        if fallback:
            return fallback.decode("utf-8")
        return ""
    if ext == ".rtf":
        converted = _pandoc_convert(data_bytes, ".rtf", "md")
        if converted:
            return converted.decode("utf-8")
        return _decode_text_full(data_bytes)
    return ""

# ==================== æ•°æ®åº“æ“ä½œ ====================
def get_db_connection():
    db_path = get_storage_paths()["db_path"]
    conn = sqlite3.connect(db_path, timeout=30)
    try:
        conn.execute("PRAGMA journal_mode=WAL;")
    except Exception:
        pass
    return conn

def init_and_migrate_db():
    conn = get_db_connection()
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS tasks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date TEXT, task_name TEXT, category TEXT, is_done INTEGER, details TEXT, tags TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    # æ·»åŠ å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    c.execute("PRAGMA table_info(tasks)")
    cols = [i[1] for i in c.fetchall()]
    if 'tags' not in cols:
        try:
            c.execute("ALTER TABLE tasks ADD COLUMN tags TEXT DEFAULT ''")
            conn.commit()
        except:
            pass
    conn.commit()
    conn.close()

def auto_backup():
    paths = get_storage_paths()
    db_path = paths["db_path"]
    backup_dir = paths["backup_dir"]
    if os.path.exists(db_path):
        d_str = datetime.now().strftime("%Y-%m-%d")
        bk_p = os.path.join(backup_dir, f"lab_data_{d_str}.db")
        if not os.path.exists(bk_p):
            try:
                shutil.copy(db_path, bk_p)
            except:
                pass

def run_query(q, p=(), fetch=False):
    conn = get_db_connection()
    c = conn.cursor()
    c.execute(q, p)
    if fetch:
        d = c.fetchall()
        cols = [desc[0] for desc in c.description]
        conn.close()
        return pd.DataFrame(d, columns=cols)
    conn.commit()
    conn.close()

def insert_task_record(date_str: str, task_name: str, category: str, details: str, tags: str) -> int:
    """æ’å…¥ä¸€æ¡ä»»åŠ¡è®°å½•å¹¶è¿”å›è‡ªå¢ ID"""
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute(
        "INSERT INTO tasks (date, task_name, category, is_done, details, tags) VALUES (?, ?, ?, ?, ?, ?)",
        (date_str, task_name, category, 0, details or "", tags or "")
    )
    new_id = cur.lastrowid
    conn.commit()
    conn.close()
    return new_id

def get_distinct_tags():
    """è·å–ç°æœ‰æ ‡ç­¾ä¸‹æ‹‰é€‰é¡¹"""
    df = run_query("SELECT tags FROM tasks WHERE tags IS NOT NULL AND TRIM(tags)!=''", fetch=True)
    if df.empty:
        return []
    tags = set()
    for raw in df["tags"]:
        if not raw:
            continue
        parts = re.split(r"[,ï¼Œ\\s]+", str(raw))
        for part in parts:
            part = part.strip()
            if part:
                tags.add(part)
    return sorted(tags)

# ==================== ä¼˜åŒ–çš„å†å²è®°å½•å¯¼å…¥ ====================
def import_legacy_records_preserve_original(files, *, default_category: str, default_tags: str, default_date, prefer_filename_date: bool = True, use_ai_metadata: bool = True):
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šä¿ç•™åŸå§‹è®°å½•å†…å®¹ï¼ŒAIåªæå–å…ƒæ•°æ®
    """
    results = []
    if not files:
        return results
    
    if isinstance(default_date, datetime):
        fallback_date = default_date
    else:
        fallback_date = datetime.combine(default_date, datetime.min.time())
    
    client = get_ai_client() if use_ai_metadata else None
    
    for file_item in files:
        name = getattr(file_item, "name", "legacy_record")
        ext = os.path.splitext(name)[1].lower()
        data = None
        
        try:
            if hasattr(file_item, "getvalue"):
                data = file_item.getvalue()
            elif hasattr(file_item, "read"):
                data = file_item.read()
        except Exception:
            data = None
        
        if not data:
            results.append({"file": name, "success": False, "message": "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"})
            continue
        
        try:
            # æå–åŸå§‹æ–‡æœ¬å†…å®¹
            if ext in (".md", ".markdown", ".txt", ".csv", ".tsv"):
                original_text = _decode_text_full(data)
                if ext in (".csv", ".tsv"):
                    original_text = f"```\n{original_text}\n```"
            elif ext in (".docx", ".doc", ".rtf"):
                original_text = convert_document_bytes_to_markdown(data, name, ext)
            elif ext in LEGACY_IMAGE_EXTS:
                results.append({"file": name, "success": False, "message": "è¯·å°†å›¾ç‰‡åµŒå…¥æ–‡æ¡£ä¸€èµ·å¯¼å…¥"})
                continue
            else:
                original_text = _decode_text_full(data)
            
            original_text = (original_text or "").strip()
            if not original_text:
                results.append({"file": name, "success": False, "message": "æœªè§£æå‡ºå†…å®¹"})
                continue
            
            # æå–å…ƒæ•°æ®
            date_str = guess_record_date_from_filename(name, fallback_date) if prefer_filename_date else fallback_date.strftime("%Y-%m-%d")
            task_name = build_task_name_from_filename(name)
            category = default_category
            tags = default_tags
            
            # ä½¿ç”¨AIæå–æ›´å‡†ç¡®çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            if client and use_ai_metadata:
                metadata = ai_extract_metadata(client, original_text[:1000])  # åªåˆ†æå‰1000å­—ç¬¦
                if metadata:
                    task_name = metadata.get('task_name', task_name)
                    category = metadata.get('category', category)
                    tags = metadata.get('tags', tags)
                    # å¦‚æœAIæå–äº†æ—¥æœŸï¼Œä½¿ç”¨å®ƒ
                    if metadata.get('date'):
                        try:
                            datetime.strptime(metadata['date'], '%Y-%m-%d')
                            date_str = metadata['date']
                        except:
                            pass
            
            # æ’å…¥è®°å½•ï¼ŒåŸå§‹å†…å®¹ä¸€å­—ä¸æ”¹
            new_id = insert_task_record(date_str, task_name, category, original_text, tags)
            results.append({
                "file": name, 
                "success": True, 
                "task_id": new_id, 
                "date": date_str,
                "task_name": task_name,
                "category": category,
                "tags": tags,
                "content_preview": original_text[:100] + "..." if len(original_text) > 100 else original_text
            })
            
        except Exception as exc:
            results.append({"file": name, "success": False, "message": str(exc)})
    
    return results

# ==================== å¯¼å‡ºåŠŸèƒ½ ====================
def build_record_markdown(row):
    row = normalize_task_row(row)
    details = row.get("details") or "(æš‚æ— å®éªŒè®°å½•)"
    md = [
        f"# {row.get('task_name', 'å®éªŒè®°å½•')}",
        "",
        f"- æ—¥æœŸï¼š{row.get('date', '-')}",
        f"- ç±»å‹ï¼š{row.get('category', '-')}",
        f"- æ ‡ç­¾ï¼š{row.get('tags') or '-'}",
        "",
        "## å®éªŒè®°å½•",
        details
    ]
    return "\n".join(md)

def build_record_docx_bytes(row):
    row = normalize_task_row(row)
    doc = Document()
    doc.add_heading(row.get("task_name", "å®éªŒè®°å½•"), level=1)
    doc.add_paragraph(f"æ—¥æœŸï¼š{row.get('date', '-')}")
    doc.add_paragraph(f"ç±»å‹ï¼š{row.get('category', '-')}")
    doc.add_paragraph(f"æ ‡ç­¾ï¼š{row.get('tags') or '-'}")
    doc.add_heading("å®éªŒè®°å½•", level=2)
    doc.add_paragraph(row.get("details") or "(æš‚æ— å®éªŒè®°å½•)")
    bio = BytesIO()
    doc.save(bio)
    return bio.getvalue()

def get_record_exports(row):
    row = normalize_task_row(row)
    base = sanitize_filename(f"{row.get('date', '')}_{row.get('task_name', 'record')}")
    markdown_bytes = build_record_markdown(row).encode("utf-8")
    docx_bytes = build_record_docx_bytes(row)
    return [
        ("MD", f"{base}.md", markdown_bytes, "text/markdown"),
        ("DOCX", f"{base}.docx", docx_bytes, "application/vnd.openxmlformats-officedocument.wordprocessingml.document"),
    ]

# ==================== Streamlit UI ç»„ä»¶ ====================
@st.dialog("ğŸ“… å¿«é€Ÿæ·»åŠ æ—¥ç¨‹", width="small")
def show_add_task_dialog(default_date_str):
    """å¿«é€Ÿæ·»åŠ ä»»åŠ¡å¯¹è¯æ¡†"""
    try:
        dd = datetime.strptime(default_date_str, "%Y-%m-%d").date()
    except:
        dd = datetime.now().date()
    
    with st.form("quick_add"):
        st.write(f"æ—¥æœŸï¼š**{dd}**")
        col1, col2 = st.columns([3, 1])
        task_name = col1.text_input("å†…å®¹", placeholder="ä»»åŠ¡åç§°")
        category = col2.selectbox("ç±»å‹", ["ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"])
        tags = st.text_input("æ ‡ç­¾", "#æ—¥å¸¸")
        
        if st.form_submit_button("æ·»åŠ ", use_container_width=True):
            if task_name.strip():
                run_query(
                    "INSERT INTO tasks (date, task_name, category, is_done, details, tags) VALUES (?,?,?,?,?,?)",
                    (dd, task_name.strip(), category, 0, "", tags.strip())
                )
                st.success("âœ… ä»»åŠ¡å·²æ·»åŠ ")
                time.sleep(0.5)
                st.rerun()

@st.dialog("ğŸ“Œ ä»»åŠ¡è¯¦æƒ…", width="medium")
def show_event_action_dialog(task_id):
    """ä»»åŠ¡è¯¦æƒ…å¯¹è¯æ¡†"""
    df = run_query("SELECT * FROM tasks WHERE id=?", (task_id,), fetch=True)
    if df.empty:
        st.error("æœªæ‰¾åˆ°è¯¥ä»»åŠ¡")
        return
    
    row = df.iloc[0]
    st.markdown(f"### {row['task_name']}")
    
    record_status = "âœ… å·²å¡«å†™å®éªŒè®°å½•" if (row['details'] or "").strip() else "ğŸ•’ æš‚æœªå¡«å†™å®éªŒè®°å½•"
    st.caption(f"{record_status} Â· æ ‡ç­¾ï¼š{row['tags'] or '-'}")
    
    try:
        date_value = datetime.strptime(str(row['date']), "%Y-%m-%d").date()
    except:
        date_value = datetime.now().date()
    
    with st.form(f"edit_task_{task_id}"):
        name = st.text_input("ä»»åŠ¡åç§°", row['task_name'])
        date_input = st.date_input("æ—¥æœŸ", value=date_value)
        category_options = ["ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"]
        category = st.selectbox("ç±»å‹", category_options, 
                               index=category_options.index(row['category']) if row['category'] in category_options else 0)
        tags = st.text_input("æ ‡ç­¾", row['tags'] or "")
        
        submitted = st.form_submit_button("ä¿å­˜ä¿®æ”¹", use_container_width=True)
        if submitted:
            run_query(
                "UPDATE tasks SET date=?, task_name=?, category=?, tags=? WHERE id=?",
                (date_input.strftime("%Y-%m-%d"), name, category, tags, task_id)
            )
            st.success("âœ… ä»»åŠ¡å·²æ›´æ–°")
            time.sleep(0.5)
            st.rerun()
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ åˆ é™¤ä»»åŠ¡", type="secondary", use_container_width=True):
            run_query("DELETE FROM tasks WHERE id=?", (task_id,))
            st.success("âœ… å·²åˆ é™¤")
            time.sleep(0.5)
            st.rerun()
    with col2:
        if st.button("ğŸ“ ç¼–è¾‘å®éªŒè®°å½•", type="primary", use_container_width=True):
            st.session_state["open_record_editor_task_id"] = int(task_id)
            st.rerun()

@st.dialog("ğŸ§ª ç¼–è¾‘å®éªŒè®°å½•", width="large")
def show_record_editor_dialog(task_id: int):
    """å®éªŒè®°å½•ç¼–è¾‘å™¨"""
    df = run_query("SELECT * FROM tasks WHERE id=?", (task_id,), fetch=True)
    if df.empty:
        st.error("æœªæ‰¾åˆ°è¯¥ä»»åŠ¡")
        return
    
    row = df.iloc[0]
    st.markdown(f"### [{row['date']}] {row['task_name']}")
    st.caption(f"ç±»å‹ï¼š{row['category'] or '-'} Â· å½“å‰æ ‡ç­¾ï¼š{row['tags'] or '-'}")
    
    # åˆå§‹åŒ–çŠ¶æ€
    tags_key = f"record_tags_{task_id}"
    details_key = f"record_details_{task_id}"
    init_key = f"record_dialog_init_{task_id}"
    
    if not st.session_state.get(init_key):
        st.session_state[tags_key] = row['tags'] or ""
        st.session_state[details_key] = row['details'] or ""
        st.session_state[init_key] = True
    
    col_main, col_side = st.columns([2, 1])
    
    with col_main:
        st.text_input("æ ‡ç­¾", key=tags_key)
        st.text_area("å®éªŒè®°å½•å†…å®¹", key=details_key, height=350)
        
        # AIæ¶¦è‰²åŒºåŸŸ
        st.markdown("#### âœ¨ AI æ¶¦è‰²åŠ©æ‰‹")
        polish_key = f"polish_result_{task_id}"
        feedback_key = f"polish_feedback_{task_id}"
        
        if polish_key not in st.session_state:
            st.session_state[polish_key] = ""
        if feedback_key not in st.session_state:
            st.session_state[feedback_key] = ""
        
        st.text_area("è¡¥å……è¦æ±‚/åé¦ˆ", key=feedback_key, height=80)
        
        col_btn1, col_btn2 = st.columns(2)
        with col_btn1:
            if st.button("âœ¨ åˆæ¬¡æ¶¦è‰²", key=f"ai_polish_{task_id}"):
                base_text = st.session_state[details_key]
                if not base_text.strip():
                    st.warning("è¯·å…ˆå¡«å†™å†…å®¹")
                else:
                    client = get_ai_client()
                    if client:
                        extra = (st.session_state[feedback_key] or "").strip() or None
                        with st.spinner("AI æ­£åœ¨æ¶¦è‰²..."):
                            res = ai_polish_text(client, base_text, extra_instruction=extra)
                        if "Error" not in res:
                            st.session_state[polish_key] = res
                            st.success("âœ¨ æ¶¦è‰²å®Œæˆ")
                        else:
                            st.error(res)
        
        with col_btn2:
            disabled = not st.session_state[polish_key]
            if st.button("ğŸª„ æ ¹æ®åé¦ˆå†æ¶¦è‰²", key=f"ai_repolish_{task_id}", disabled=disabled):
                client = get_ai_client()
                if client:
                    extra = (st.session_state[feedback_key] or "").strip() or None
                    base_text = st.session_state[polish_key] or st.session_state[details_key]
                    with st.spinner("AI æ­£åœ¨æ ¹æ®åé¦ˆæ¶¦è‰²..."):
                        res = ai_polish_text(client, base_text, extra_instruction=extra)
                    if "Error" not in res:
                        st.session_state[polish_key] = res
                        st.success("âœ… å·²æ ¹æ®åé¦ˆæ›´æ–°")
                    else:
                        st.error(res)
        
        if st.button("ğŸ’¾ ä¿å­˜è®°å½•", type="primary", use_container_width=True):
            run_query(
                "UPDATE tasks SET details=?, tags=? WHERE id=?",
                (st.session_state[details_key], st.session_state[tags_key], task_id)
            )
            st.success("âœ… å·²ä¿å­˜")
            st.session_state[init_key] = False
            time.sleep(0.5)
            st.rerun()
        
        if st.session_state[polish_key]:
            st.text_area("AI æ¶¦è‰²ç»“æœ", st.session_state[polish_key], height=300)
    
    with col_side:
        st.info("ğŸ“ é™„ä»¶ä¸Šä¼ ")
        uploads = st.file_uploader("é€‰æ‹©æ–‡ä»¶", accept_multiple_files=True, key=f"upload_{task_id}")
        if uploads:
            for f in uploads:
                save_path, display_name = get_versioned_upload_path(f.name)
                with open(save_path, "wb") as w:
                    w.write(f.getbuffer())
                snippet = f"![{display_name}]({save_path})" if f.type and f.type.startswith("image") else f"[{display_name}]({save_path})"
                st.code(snippet)

@st.dialog("ğŸ¤– AI ä»»åŠ¡é¢„è§ˆä¸ç¡®è®¤", width="large")
def show_ai_confirm_dialog(tasks_data):
    """AIä»»åŠ¡ç¡®è®¤å¯¹è¯æ¡†"""
    st.info("AI æ ¹æ®æ‚¨çš„æè¿°ç”Ÿæˆäº†ä»¥ä¸‹è®¡åˆ’ã€‚è¯·å‹¾é€‰éœ€è¦å¯¼å…¥çš„é¡¹ç›®ï¼Œä¹Ÿå¯ç›´æ¥ä¿®æ”¹å†…å®¹ã€‚")
    
    df = pd.DataFrame(tasks_data)
    if 'task_name' in df.columns:
        df['task_name'] = df['task_name'].apply(lambda x: shorten_task_name(x))
    
    if 'import' not in df.columns:
        df.insert(0, 'import', True)
    
    df['date'] = pd.to_datetime(df['date']).dt.date
    if 'record_outline' not in df.columns:
        df['record_outline'] = ""
    
    edited_df = st.data_editor(
        df,
        column_config={
            "import": st.column_config.CheckboxColumn("å¯¼å…¥?", width="small"),
            "date": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD"),
            "task_name": st.column_config.TextColumn("ä»»åŠ¡åç§°"),
            "category": st.column_config.SelectboxColumn("ç±»å‹", options=["ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"]),
            "tags": st.column_config.TextColumn("æ ‡ç­¾"),
            "record_outline": st.column_config.TextColumn("å®éªŒè®°å½•è¦ç‚¹", help="å°†åŒæ­¥å†™å…¥ä»»åŠ¡çš„å®éªŒè®°å½•è¯¦æƒ…")
        },
        hide_index=True,
        use_container_width=True,
        num_rows="dynamic"
    )
    
    if st.button("ğŸš€ ç¡®è®¤ä¸€é”®å¯¼å…¥", type="primary", use_container_width=True):
        count = 0
        for index, row in edited_df.iterrows():
            if row['import']:
                outline = row.get('record_outline', "")
                if pd.isna(outline):
                    outline = ""
                run_query(
                    "INSERT INTO tasks (date, task_name, category, is_done, details, tags) VALUES (?, ?, ?, ?, ?, ?)",
                    (row['date'], row['task_name'], row['category'], 0, outline, row['tags'])
                )
                count += 1
        st.success(f"âœ… æˆåŠŸå¯¼å…¥ {count} æ¡ä»»åŠ¡ï¼")
        time.sleep(1)
        st.rerun()

# ==================== ä¸»åº”ç”¨ ====================
def setup_page_config():
    """è®¾ç½®é¡µé¢é…ç½®"""
    st.set_page_config(
        page_title="Lab Diary AI - æ™ºèƒ½å®éªŒè®°å½•å·¥å…·",
        layout="wide",
        initial_sidebar_state="expanded",
        page_icon="ğŸ”¬"
    )
    
    # è‡ªå®šä¹‰CSS
    st.markdown(f"""
    <style>
    /* å…¨å±€æ ·å¼ */
    .stApp {{
        font-family: {FONTS['family']};
        background-color: {COLORS['background']};
    }}
    
    /* æ ‡é¢˜æ ·å¼ */
    h1, h2, h3 {{
        color: {COLORS['primary']};
        font-weight: 600;
    }}
    
    /* æŒ‰é’®æ ·å¼ */
    .stButton > button {{
        border-radius: 8px;
        font-weight: 500;
        transition: all 0.3s ease;
    }}
    
    .stButton > button:hover {{
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }}
    
    /* å¡ç‰‡æ ·å¼ */
    .stCard {{
        border-radius: 12px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        transition: all 0.3s ease;
    }}
    
    .stCard:hover {{
        transform: translateY(-4px);
        box-shadow: 0 8px 24px rgba(0,0,0,0.15);
    }}
    
    /* è¾“å…¥æ¡†æ ·å¼ */
    .stTextInput > div > div > input {{
        border-radius: 8px;
        border: 1px solid #e2e8f0;
    }}
    
    .stTextInput > div > div > input:focus {{
        border-color: {COLORS['accent']};
        box-shadow: 0 0 0 2px rgba(59, 130, 246, 0.2);
    }}
    
    /* ä¾§è¾¹æ æ ·å¼ */
    [data-testid="stSidebar"] {{
        background-color: white;
        box-shadow: 2px 0 8px rgba(0,0,0,0.1);
    }}
    
    /* æ—¥å†æ ·å¼è¦†ç›– */
    .fc-daygrid-event {{
        height: auto !important;
    }}
    
    .fc-daygrid-event .fc-event-main,
    .fc-daygrid-event .fc-event-title {{
        white-space: normal !important;
        overflow-wrap: anywhere;
        line-height: 1.2;
    }}
    
    /* å“åº”å¼è®¾è®¡ */
    @media (max-width: 768px) {{
        [data-testid="stSidebar"] {{
            width: 100% !important;
        }}
    }}
    </style>
    """, unsafe_allow_html=True)

def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    with st.sidebar:
        storage = get_storage_paths()
        # Logoå’Œæ ‡é¢˜
        st.markdown(f"""
        <div style="text-align: center; padding: 20px 0;">
            <h1 style="color: {COLORS['primary']}; font-size: 24px; margin: 0;">ğŸ”¬ Lab Diary AI</h1>
            <p style="color: {COLORS['secondary']}; font-size: 12px; margin: 5px 0 0 0;">æ™ºèƒ½å®éªŒè®°å½•ç®¡ç†</p>
        </div>
        """, unsafe_allow_html=True)
        if storage.get("user_label") and storage["user_label"] != "local":
            st.caption(f"å½“å‰ç”¨æˆ·ï¼š{storage['user_label']}")
        
        st.divider()
        
        # å¯¼èˆªèœå•
        nav_pages = ["ğŸ“… æ—¥å†æ€»è§ˆ", "ğŸ“– å®éªŒè®°å½•"]
        page = st.radio("å¯¼èˆª", nav_pages, key="nav_page")
        
        st.divider()
        
        # AIåŠ©æ‰‹é¢æ¿
        st.markdown(f"""
        <div style="background: linear-gradient(135deg, {COLORS['accent']}20, {COLORS['info']}20); 
                    padding: 16px; border-radius: 12px; margin-bottom: 16px;">
            <h3 style="color: {COLORS['primary']}; margin: 0 0 8px 0;">ğŸ¤– AI æ™ºèƒ½åŠ©æ‰‹</h3>
            <p style="color: {COLORS['secondary']}; font-size: 12px; margin: 0;">
                ç”¨è‡ªç„¶è¯­è¨€æè¿°æ‚¨çš„è®¡åˆ’ï¼ŒAIå°†è‡ªåŠ¨åˆ›å»ºä»»åŠ¡
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # AIè¾“å…¥
        prompt_key = "ai_schedule_prompt"
        user_prompt = st.text_area(
            "æè¿°æ‚¨çš„è®¡åˆ’",
            key=prompt_key,
            height=120,
            placeholder="ä¾‹å¦‚ï¼šæ˜å¤©å¼€å§‹è¿ç»­3å¤©æµ‹ä½“é‡ï¼Œä¸‹å‘¨äº”å¤„æ­»å–è„‘"
        )
        
        # å‚è€ƒæ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader(
            "å‚è€ƒæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰",
            accept_multiple_files=True,
            key="ai_schedule_files"
        )
        
        # ç”Ÿæˆä»»åŠ¡æŒ‰é’®
        if st.button("âš¡ ç”Ÿæˆä»»åŠ¡é¢„è§ˆ", type="primary", use_container_width=True):
            if not user_prompt.strip():
                st.warning("è¯·å…ˆè¾“å…¥è®¡åˆ’æè¿°")
            else:
                client = get_ai_client()
                if not client:
                    st.error("AI æœåŠ¡æœªé…ç½®")
                else:
                    with st.spinner("AI æ­£åœ¨åˆ†æ..."):
                        tasks = ai_parse_schedule(client, user_prompt)
                        if tasks:
                            for task in tasks:
                                task['task_name'] = shorten_task_name(task.get('task_name', ''))
                            show_ai_confirm_dialog(tasks)
                        else:
                            st.error("AI æœªèƒ½è¯†åˆ«å‡ºä»»åŠ¡ï¼Œè¯·å°è¯•æ¢ä¸ªè¯´æ³•")
        
        return page

def render_calendar_page():
    """æ¸²æŸ“æ—¥å†é¡µé¢"""
    st.markdown(f"""
    <div style="background: white; padding: 24px; border-radius: 16px; box-shadow: 0 4px 12px rgba(0,0,0,0.05);">
        <h1 style="color: {COLORS['primary']}; margin: 0 0 16px 0;">ğŸ“… å·¥ä½œæ—¥ç¨‹æ€»è§ˆ</h1>
    </div>
    """, unsafe_allow_html=True)
    
    # æœç´¢æ 
    col_search, col_filter = st.columns([3, 1])
    search_term = col_search.text_input(
        "ğŸ” æœç´¢",
        key="calendar_search",
        placeholder="ä»»åŠ¡åç§° / å®éªŒè®°å½• / æ ‡ç­¾"
    )
    category_filter = col_filter.selectbox(
        "ç±»åˆ«ç­›é€‰",
        ["å…¨éƒ¨", "ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"],
        key="calendar_category"
    )
    
    # æ—¥å†é…ç½®
    cal_ops = {
        "headerToolbar": {
            "left": "today prev,next",
            "center": "title",
            "right": "dayGridMonth,dayGridWeek,dayGridDay"
        },
        "initialView": "dayGridMonth",
        "timeZone": "UTC",
        "buttonText": {"today": "ä»Šå¤©", "dayGridMonth": "æœˆ", "dayGridWeek": "å‘¨", "dayGridDay": "æ—¥"},
        "selectable": True,
        "navLinks": False,
        "editable": False,
        "height": 600
    }
    
    # è·å–ä»»åŠ¡æ•°æ®ï¼ˆæ”¯æŒæœç´¢/ç­›é€‰ï¼‰
    where_parts = []
    params: list = []
    if search_term:
        wildcard = f"%{search_term}%"
        where_parts.append("(task_name LIKE ? OR details LIKE ? OR tags LIKE ?)")
        params.extend([wildcard, wildcard, wildcard])
    if category_filter and category_filter != "å…¨éƒ¨":
        where_parts.append("category=?")
        params.append(category_filter)

    where_sql = (" WHERE " + " AND ".join(where_parts)) if where_parts else ""
    df = run_query("SELECT * FROM tasks" + where_sql + " ORDER BY date", tuple(params), fetch=True)
    df_list = run_query("SELECT * FROM tasks" + where_sql + " ORDER BY date DESC, id DESC", tuple(params), fetch=True)
    events = []
    
    if not df.empty:
        category_color = {
            "ç§‘ç ”": COLORS["research"],
            "ä¸´åºŠ": COLORS["clinical"],
            "è¯¾ç¨‹": COLORS["course"],
            "å…¶ä»–": COLORS["other"],
        }
        for _, r in df.iterrows():
            task_id = int(r['id'])
            color = category_color.get(str(r.get("category", "")).strip(), COLORS["other"])
            
            details_text = (r['details'] or "").strip()
            record_done = bool(details_text)
            prefix = "âœ… " if record_done else "â¬œ "
            
            event = {
                "id": str(task_id),
                "title": prefix + r['task_name'],
                "start": r['date'],
                "backgroundColor": color,
                "borderColor": color,
                "allDay": True,
                "extendedProps": {
                    "task_id": task_id,
                    "task_name": r['task_name'],
                    "date": r['date'],
                    "category": r['category'],
                    "tags": r['tags'] or "",
                    "is_done": bool(r['is_done']),
                    "details_filled": record_done,
                    "details_preview": details_text[:80] + "..." if len(details_text) > 80 else details_text
                }
            }
            events.append(event)
    
    # æ¸²æŸ“æ—¥å†ï¼ˆç§»é™¤å³ä¾§å¿«é€Ÿç»Ÿè®¡æ ï¼‰
    cal = calendar(
        events=events,
        options=cal_ops,
        callbacks=['dateClick', 'eventClick', 'eventMouseEnter'],
        key='main_calendar'
    )

    # å¤„ç†æ—¥å†å›è°ƒ
    callback_type = cal.get("callback")
    if callback_type == "dateClick":
        d_str = cal["dateClick"].get("dateStr") or cal["dateClick"].get("date")
        if d_str:
            if "T" in d_str:
                d_str = d_str.split("T")[0]
            show_add_task_dialog(d_str)
    elif callback_type == "eventClick":
        event_payload = cal.get("eventClick", {}).get("event", {})
        props = event_payload.get("extendedProps", {})
        task_id = props.get("task_id") or event_payload.get("id")
        if task_id is not None:
            show_event_action_dialog(int(str(task_id)))

    st.divider()
    st.subheader("ğŸ“‹ ä»»åŠ¡æ€»åˆ—è¡¨")

    with st.form("task_quick_add"):
        c1, c2, c3, c4 = st.columns([2, 1, 1, 1])
        new_name = c1.text_input("ä»»åŠ¡åç§°", placeholder="ä¾‹å¦‚ï¼šæµ‹ä½“é‡/çŒèƒƒ/è¡Œä¸ºå­¦æµ‹è¯•")
        new_date = c2.date_input("æ—¥æœŸ", value=datetime.now().date())
        new_category = c3.selectbox("ç±»å‹", ["ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"])
        new_tags = c4.text_input("æ ‡ç­¾", value="#æ—¥å¸¸")
        new_details = st.text_input("å¤‡æ³¨ï¼ˆå¯é€‰ï¼‰")
        submitted = st.form_submit_button("æ·»åŠ ä»»åŠ¡", use_container_width=True)
        if submitted:
            if not new_name.strip():
                st.warning("è¯·å¡«å†™ä»»åŠ¡åç§°")
            else:
                run_query(
                    "INSERT INTO tasks (date, task_name, category, is_done, details, tags) VALUES (?, ?, ?, ?, ?, ?)",
                    (new_date.strftime("%Y-%m-%d"), new_name.strip(), new_category, 0, new_details.strip(), new_tags.strip())
                )
                st.success("ä»»åŠ¡å·²æ·»åŠ ")
                st.rerun()

    if df_list.empty:
        st.info("æš‚æ— ä»»åŠ¡æ•°æ®")
        return

    df_list['date'] = pd.to_datetime(df_list['date']).dt.date
    header_cols = st.columns([0.7, 1.1, 0.8, 1.3, 3, 1.6])
    header_cols[0].markdown("**å®Œæˆ**")
    header_cols[1].markdown("**æ—¥æœŸ**")
    header_cols[2].markdown("**ç±»å‹**")
    header_cols[3].markdown("**æ ‡ç­¾/è®°å½•**")
    header_cols[4].markdown("**å†…å®¹**")
    header_cols[5].markdown("**æ“ä½œ**")

    for _, row in df_list.iterrows():
        with st.container():
            c1, c2, c3, c4, c5, c6 = st.columns([0.7, 1.1, 0.8, 1.3, 3, 1.6])
            done_val = bool(row['is_done'])
            checked = c1.checkbox("", done_val, key=f"task_done_{row['id']}")
            if checked != done_val:
                run_query("UPDATE tasks SET is_done=? WHERE id=?", (1 if checked else 0, row['id']))
                st.rerun()

            c2.text(str(row['date']))
            c3.caption(row['category'])
            record_flag = "âœ… å·²å†™" if (row['details'] or "").strip() else "âœï¸ å¾…å†™"
            c4.markdown(f"{row['tags'] or '-'} Â· {record_flag}")
            if row['is_done']:
                c5.markdown(f"~~{row['task_name']}~~")
            else:
                c5.text(row['task_name'])

            action_col, record_col = c6.columns(2)
            if action_col.button("è¯¦æƒ…", key=f"task_detail_{row['id']}"):
                show_event_action_dialog(int(row['id']))
            if record_col.button("è®°å½•", key=f"task_record_{row['id']}"):
                show_record_editor_dialog(int(row['id']))

            st.markdown("<hr style='margin:0.2em 0;opacity:0.1'>", unsafe_allow_html=True)

# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    setup_page_config()
    init_and_migrate_db()
    auto_backup()
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "nav_page" not in st.session_state:
        st.session_state["nav_page"] = "ğŸ“… æ—¥å†æ€»è§ˆ"
    
    # æ¸²æŸ“ä¾§è¾¹æ å¹¶è·å–å½“å‰é¡µé¢
    page = render_sidebar()

    # é¿å… Dialog åµŒå¥—ï¼šåœ¨å¯¹è¯æ¡†å†…åªè®¾ç½®æ ‡è®°å¹¶ rerunï¼ŒçœŸæ­£æ‰“å¼€ç¼–è¾‘å™¨åœ¨ä¸»æ¸²æŸ“é˜¶æ®µå®Œæˆ
    pending_record_task_id = st.session_state.pop("open_record_editor_task_id", None)
    if pending_record_task_id is not None:
        show_record_editor_dialog(int(pending_record_task_id))
    
    # æ ¹æ®é¡µé¢æ¸²æŸ“å†…å®¹
    if page == "ğŸ“… æ—¥å†æ€»è§ˆ":
        render_calendar_page()
    elif page == "ğŸ“– å®éªŒè®°å½•":
        render_archive_page()

if __name__ == "__main__":
    main()
# ç»§ç»­æ·»åŠ ç¼ºå¤±çš„å‡½æ•°

# ==================== è¾…åŠ©å‡½æ•°ï¼ˆç»§ç»­ï¼‰ ====================
def _safe_date_from_parts(year: str, month: str, day: str) -> str | None:
    """ä»å¹´æœˆæ—¥ç»„ä»¶å®‰å…¨åˆ›å»ºæ—¥æœŸå­—ç¬¦ä¸²"""
    try:
        dt = datetime(year=int(year), month=int(month), day=int(day))
        return dt.strftime("%Y-%m-%d")
    except Exception:
        return None

def guess_record_date_from_filename(filename: str, fallback_date: datetime) -> str:
    """æ ¹æ®æ–‡ä»¶åä¸­çš„æ—¥æœŸä¿¡æ¯æ¨æµ‹æ—¥å¿—æ—¥æœŸ"""
    base = os.path.basename(filename)
    stem = os.path.splitext(base)[0]
    patterns = [
        r"(20\d{2})[-_/\.](\d{1,2})[-_/\.](\d{1,2})",
        r"(20\d{2})(\d{2})(\d{2})",
        r"(20\d{2})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",
    ]
    for pattern in patterns:
        match = re.search(pattern, stem)
        if match:
            guess = _safe_date_from_parts(*match.groups())
            if guess:
                return guess
    return fallback_date.strftime("%Y-%m-%d")

def build_task_name_from_filename(filename: str) -> str:
    """å°†æ–‡ä»¶åè½¬ä¸ºæ˜“è¯»çš„ä»»åŠ¡æ ‡é¢˜"""
    base = os.path.basename(filename)
    stem = os.path.splitext(base)[0]
    stem = re.sub(r"(20\d{2}[^\d]?\d{1,2}[^\d]?\d{1,2})", " ", stem)
    stem = re.sub(r"[_\-]+", " ", stem)
    stem = re.sub(r"\s+", " ", stem).strip()
    if not stem:
        stem = "å†å²è®°å½•"
    return shorten_task_name(stem)

# ==================== DOCX å¤„ç†å‡½æ•° ====================
def _iter_docx_block_items(parent):
    """è¿­ä»£DOCXæ–‡æ¡£ä¸­çš„å—å…ƒç´ """
    if isinstance(parent, _Cell):
        parent_element = parent._tc
    else:
        parent_element = parent.element.body
    for child in parent_element.iterchildren():
        if isinstance(child, CT_P):
            yield Paragraph(child, parent)
        elif isinstance(child, CT_Tbl):
            yield Table(child, parent)

def _paragraph_is_list(paragraph: Paragraph) -> bool:
    """åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ºåˆ—è¡¨é¡¹"""
    p = paragraph._p
    if p is None or p.pPr is None:
        return False
    return p.pPr.numPr is not None

def _paragraph_is_code(paragraph: Paragraph) -> bool:
    """åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ºä»£ç å—"""
    style_name = (paragraph.style.name or "").lower() if paragraph.style else ""
    code_keywords = ("code", "ç­‰å®½", "monospace")
    if any(token in style_name for token in code_keywords):
        return True
    for run in paragraph.runs:
        font = getattr(run, "font", None)
        if font and font.name:
            fname = font.name.lower()
            if any(token in fname for token in ("consolas", "courier", "monospace")):
                return True
    return False

def _heading_level_from_style(style_name: str) -> int:
    """ä»æ ·å¼åç§°æå–æ ‡é¢˜çº§åˆ«"""
    match = re.search(r"(\d+)", style_name)
    if match:
        try:
            return max(1, min(6, int(match.group(1))))
        except ValueError:
            pass
    return 1

def _docx_paragraph_to_markdown(paragraph: Paragraph) -> str:
    """å°†DOCXæ®µè½è½¬æ¢ä¸ºMarkdown"""
    text = paragraph.text.strip()
    if not text:
        return ""
    style_name = (paragraph.style.name or "").lower() if paragraph.style and paragraph.style.name else ""
    if "heading" in style_name or "æ ‡é¢˜" in style_name:
        level = _heading_level_from_style(style_name)
        return f"{'#' * level} {text}"
    if _paragraph_is_code(paragraph):
        return f"```\n{text}\n```"
    if _paragraph_is_list(paragraph) or "list" in style_name or "åˆ—è¡¨" in style_name:
        return f"- {text}"
    return text

def _docx_table_to_markdown(table: Table) -> str:
    """å°†DOCXè¡¨æ ¼è½¬æ¢ä¸ºMarkdown"""
    rows = []
    for row in table.rows:
        cells = []
        for cell in row.cells:
            cell_text = cell.text.strip()
            cell_text = cell_text.replace("\n", "<br>")
            cells.append(cell_text or " ")
        rows.append(cells)
    if not rows:
        return ""
    header = rows[0]
    divider = ["---"] * len(header)
    body = rows[1:] or [[]]
    lines = [
        "| " + " | ".join(header) + " |",
        "| " + " | ".join(divider) + " |",
    ]
    if body and body[0]:
        for row in body:
            padded = row + [" "] * (len(header) - len(row))
            lines.append("| " + " | ".join(padded[:len(header)]) + " |")
    return "\n".join(lines)

def _collect_docx_image_markdown(docx_bytes: bytes, origin_name: str) -> list[str]:
    """ä»DOCXä¸­æå–å›¾ç‰‡"""
    images = []
    label_prefix = sanitize_filename(os.path.splitext(origin_name)[0] or "legacy_doc")
    with zipfile.ZipFile(BytesIO(docx_bytes)) as archive:
        for entry in archive.infolist():
            if entry.is_dir():
                continue
            if not entry.filename.startswith("word/media/"):
                continue
            data = archive.read(entry.filename)
            img_name = os.path.basename(entry.filename)
            images.append(_persist_image_as_markdown(data, f"{label_prefix}_{img_name}"))
    return images

def _pandoc_convert(data_bytes: bytes, source_ext: str, target: str) -> bytes | None:
    """ä½¿ç”¨Pandocè½¬æ¢æ–‡æ¡£"""
    if not HAS_PYPANDOC:
        return None
    suffix = source_ext if source_ext.startswith(".") else f".{source_ext}"
    tmp_in = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    tmp_in.write(data_bytes)
    tmp_in.close()
    try:
        if target == "docx":
            tmp_out = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
            tmp_out_path = tmp_out.name
            tmp_out.close()
            pypandoc.convert_file(tmp_in.name, to="docx", format=source_ext.lstrip("."), outputfile=tmp_out_path)
            with open(tmp_out_path, "rb") as fh:
                return fh.read()
        else:
            result = pypandoc.convert_file(tmp_in.name, to=target, format=source_ext.lstrip("."))
            return result.encode("utf-8")
    except (OSError, RuntimeError):
        return None
    finally:
        try:
            os.remove(tmp_in.name)
        except OSError:
            pass

def _convert_doc_via_win32(data_bytes: bytes) -> bytes | None:
    """åœ¨Windowsç¯å¢ƒä¸‹å°†DOCè½¬ä¸ºDOCX"""
    if not HAS_WIN32_COM:
        return None
    temp_dir = tempfile.mkdtemp()
    doc_path = os.path.join(temp_dir, "legacy.doc")
    docx_path = os.path.join(temp_dir, "legacy.docx")
    with open(doc_path, "wb") as fh:
        fh.write(data_bytes)
    converted = None
    word = None
    doc_obj = None
    initialized = False
    try:
        pythoncom.CoInitialize()
        initialized = True
        word = win32com.client.Dispatch("Word.Application")
        word.Visible = False
        doc_obj = word.Documents.Open(doc_path)
        doc_obj.SaveAs(docx_path, FileFormat=16)
        doc_obj.Close(False)
        with open(docx_path, "rb") as fh:
            converted = fh.read()
    except Exception:
        converted = None
    finally:
        if doc_obj is not None:
            try:
                doc_obj.Close(False)
            except Exception:
                pass
        if word is not None:
            try:
                word.Quit()
            except Exception:
                pass
        if initialized:
            try:
                pythoncom.CoUninitialize()
            except Exception:
                pass
        for path in (doc_path, docx_path):
            if os.path.exists(path):
                try:
                    os.remove(path)
                except OSError:
                    pass
        try:
            os.rmdir(temp_dir)
        except OSError:
            pass
    return converted

# ==================== å½’æ¡£é¡µé¢ ====================
def render_archive_page():
    """æ¸²æŸ“å®éªŒè®°å½•é¡µé¢"""
    st.markdown(f"""
    <div style="background: white; padding: 24px; border-radius: 16px; box-shadow: 0 4px 12px rgba(0,0,0,0.05);">
        <h1 style="color: {COLORS['primary']}; margin: 0 0 16px 0;">ğŸ“– å®éªŒè®°å½•</h1>
        <p style="color: {COLORS['secondary']}; margin: 0;">ç®¡ç†å’Œå¯¼å‡ºæ‚¨çš„å®éªŒè®°å½•</p>
    </div>
    """, unsafe_allow_html=True)
    
    # æœç´¢å’Œç­›é€‰
    col_search, col_tag = st.columns([3, 1])
    search_term = col_search.text_input("ğŸ” æœç´¢", key="archive_search")
    
    tags_available = ["å…¨éƒ¨"] + get_distinct_tags()
    tag_choice = col_tag.selectbox("æ ‡ç­¾ç­›é€‰", tags_available, key="archive_tag")
    
    # ä¸€é”®è¿ç§»å†å²è®°å½•
    with st.expander("ğŸª„ ä¸€é”®è¿ç§»å†å²è®°å½•", expanded=False):
        st.caption("æ”¯æŒ Markdown / Word / TXT / CSV ç­‰æ ¼å¼ï¼Œä¿ç•™åŸå§‹è®°å½•å†…å®¹")
        
        legacy_files = st.file_uploader(
            "é€‰æ‹©æ—§å®éªŒè®°å½•æ–‡ä»¶",
            accept_multiple_files=True,
            type=["md", "markdown", "txt", "csv", "tsv", "doc", "docx", "rtf"],
            key="legacy_import_files"
        )
        
        col1, col2, col3 = st.columns(3)
        legacy_category = col1.selectbox("å¯¼å…¥ç±»åˆ«", ["ç§‘ç ”", "ä¸´åºŠ", "è¯¾ç¨‹", "å…¶ä»–"], key="legacy_category")
        legacy_tags = col2.text_input("ç»Ÿä¸€æ ‡ç­¾", "#å†å²è®°å½•", key="legacy_tags")
        legacy_date = col3.date_input("é»˜è®¤æ—¥æœŸ", datetime.now().date(), key="legacy_date")
        
        use_ai = st.checkbox("ä½¿ç”¨AIæå–å…ƒæ•°æ®ï¼ˆæ¨èï¼‰", value=True, key="use_ai_metadata")
        filename_date = st.checkbox("å°è¯•æ ¹æ®æ–‡ä»¶åæ¨æ–­æ—¥æœŸ", value=True, key="filename_date")
        
        if st.button("ğŸš€ å¼€å§‹è¿ç§»", type="primary", use_container_width=True):
            if not legacy_files:
                st.warning("è¯·å…ˆé€‰æ‹©è‡³å°‘ä¸€ä¸ªæ–‡ä»¶")
            else:
                with st.spinner("æ­£åœ¨è§£æå¹¶å¯¼å…¥å†å²è®°å½•..."):
                    import_results = import_legacy_records_preserve_original(
                        legacy_files,
                        default_category=legacy_category,
                        default_tags=legacy_tags,
                        default_date=legacy_date,
                        prefer_filename_date=filename_date,
                        use_ai_metadata=use_ai
                    )
                
                # æ˜¾ç¤ºç»“æœ
                success_items = [item for item in import_results if item.get("success")]
                failure_items = [item for item in import_results if not item.get("success")]
                
                if success_items:
                    st.success(f"âœ… æˆåŠŸå¯¼å…¥ {len(success_items)} æ¡è®°å½•")
                    for item in success_items:
                        with st.expander(f"âœ… {item['file']}"):
                            st.write(f"**ä»»åŠ¡å**: {item['task_name']}")
                            st.write(f"**æ—¥æœŸ**: {item['date']}")
                            st.write(f"**ç±»åˆ«**: {item['category']}")
                            st.write(f"**æ ‡ç­¾**: {item['tags']}")
                            st.write(f"**é¢„è§ˆ**: {item['content_preview']}")
                
                if failure_items:
                    st.error(f"âŒ {len(failure_items)} ä¸ªæ–‡ä»¶å¯¼å…¥å¤±è´¥")
                    for item in failure_items:
                        st.write(f"âš ï¸ {item['file']}: {item.get('message', 'æœªçŸ¥é”™è¯¯')}")
    
    # è‡ªåŠ¨ç”Ÿæˆå‘¨æŠ¥
    with st.expander("ğŸ—“ï¸ è‡ªåŠ¨ç”Ÿæˆå‘¨æŠ¥", expanded=False):
        reference_date = st.date_input("å‚è€ƒæ—¥æœŸ", datetime.now().date(), key="weekly_date")
        
        if st.button("ğŸ“„ ç”Ÿæˆå‘¨æŠ¥", key="weekly_btn"):
            start_date = (reference_date - timedelta(days=6)).strftime("%Y-%m-%d")
            end_date = reference_date.strftime("%Y-%m-%d")
            
            report_df = run_query(
                "SELECT date, task_name, details, tags FROM tasks WHERE category='ç§‘ç ”' AND details!='' AND date BETWEEN ? AND ? ORDER BY date",
                (start_date, end_date),
                fetch=True
            )
            
            if report_df.empty:
                st.warning("æ‰€é€‰æ—¶é—´æ®µå†…æš‚æ— å®éªŒè®°å½•")
            else:
                records = report_df.to_dict('records')
                client = get_ai_client()
                
                if client:
                    with st.spinner("AI æ­£åœ¨æ•´ç†å‘¨æŠ¥..."):
                        report_text = ai_generate_weekly_report(client, records, start_date, end_date)
                else:
                    report_text = build_weekly_report_fallback(records, start_date, end_date)
                
                st.text_area("å‘¨æŠ¥å†…å®¹", report_text, height=300)
                st.download_button(
                    "ğŸ“¥ å¯¼å‡ºå‘¨æŠ¥",
                    report_text.encode("utf-8"),
                    file_name=f"weekly_report_{end_date}.md",
                    mime="text/markdown",
                    use_container_width=True
                )
    
    # æŸ¥è¯¢è®°å½•
    base_sql = "SELECT * FROM tasks WHERE category='ç§‘ç ”' AND details!=''"
    params = []
    
    if search_term:
        wildcard = f"%{search_term}%"
        base_sql += " AND (task_name LIKE ? OR details LIKE ? OR tags LIKE ?)"
        params.extend([wildcard, wildcard, wildcard])
    
    if tag_choice and tag_choice != "å…¨éƒ¨":
        base_sql += " AND tags LIKE ?"
        params.append(f"%{tag_choice}%")
    
    df = run_query(base_sql + " ORDER BY date DESC", tuple(params), fetch=True)
    
    if df.empty:
        st.info("ğŸ“­ æš‚æ—¶æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å®éªŒè®°å½•")
    else:
        # æ‰¹é‡å¯¼å‡º
        st.markdown("### ğŸ“¤ æ‰¹é‡å¯¼å‡º")
        archive_exports = get_archive_exports(df.to_dict('records'))
        
        if archive_exports:
            exp_cols = st.columns(len(archive_exports))
            for col, item in zip(exp_cols, archive_exports):
                label, fname, data, mime = item
                with col:
                    st.download_button(
                        f"ğŸ“„ å¯¼å‡º{label}",
                        data,
                        file_name=fname,
                        mime=mime,
                        use_container_width=True
                    )
        
        st.divider()
        
        # è®°å½•åˆ—è¡¨
        st.markdown(f"### ğŸ“‹ å®éªŒè®°å½•åˆ—è¡¨ ({len(df)}æ¡)")
        
        for _, r in df.iterrows():
            with st.expander(f"**{r['date']}** | {r['task_name']}", expanded=False):
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.caption(f"ğŸ·ï¸ æ ‡ç­¾ï¼š{r['tags'] or '-'} Â· ğŸ“‚ ç±»å‹ï¼š{r['category']}")
                    st.markdown(r['details'])
                with col2:
                    if st.button("ğŸ“ ç¼–è¾‘", key=f"edit_{r['id']}", use_container_width=True):
                        show_record_editor_dialog(int(r['id']))
                    
                    # å•ä¸ªå¯¼å‡º
                    exports = get_record_exports(r)
                    for label, fname, data, mime in exports:
                        st.download_button(
                            f"ğŸ“„ {label}",
                            data,
                            file_name=fname,
                            mime=mime,
                            key=f"export_{r['id']}_{label}",
                            use_container_width=True
                        )

def render_analytics_page():
    """æ•°æ®åˆ†æé¡µé¢ï¼ˆå·²ä¸‹çº¿ï¼‰"""
    st.info("â€œæ•°æ®åˆ†æâ€åŠŸèƒ½å·²ä¸‹çº¿ï¼ˆä¸å†ç»´æŠ¤ï¼‰ã€‚")
    return
    st.markdown(f"""
    <div style="background: white; padding: 24px; border-radius: 16px; box-shadow: 0 4px 12px rgba(0,0,0,0.05);">
        <h1 style="color: {COLORS['primary']}; margin: 0 0 16px 0;">ğŸ“Š æ•°æ®åˆ†æ</h1>
        <p style="color: {COLORS['secondary']}; margin: 0;">å¯è§†åŒ–æ‚¨çš„ç§‘ç ”å·¥ä½œæ•°æ®</p>
    </div>
    """, unsafe_allow_html=True)
    
    # è·å–æ•°æ®
    df = run_query("SELECT * FROM tasks ORDER BY date", fetch=True)
    
    if df.empty:
        st.info("ğŸ“­ æš‚æ— æ•°æ®å¯ä¾›åˆ†æ")
        return
    
    # ç»Ÿè®¡å¡ç‰‡
    st.markdown("### ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
    col1, col2, col3, col4 = st.columns(4)
    
    total_tasks = len(df)
    completed_tasks = len(df[df['is_done'] == 1])
    research_tasks = len(df[df['category'] == 'ç§‘ç ”'])
    this_week_tasks = len(df[df['date'] >= (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')])
    
    with col1:
        st.metric("æ€»ä»»åŠ¡æ•°", total_tasks)
    with col2:
        st.metric("å·²å®Œæˆ", completed_tasks)
    with col3:
        st.metric("ç§‘ç ”ä»»åŠ¡", research_tasks)
    with col4:
        st.metric("æœ¬å‘¨ä»»åŠ¡", this_week_tasks)
    
    # å›¾è¡¨åŒºåŸŸ
    col_chart1, col_chart2 = st.columns(2)
    
    with col_chart1:
        st.markdown("### ğŸ“… å·¥ä½œé‡è¶‹åŠ¿")
        workload_chart = create_workload_chart(df)
        if workload_chart:
            st.plotly_chart(workload_chart, use_container_width=True)
    
    with col_chart2:
        st.markdown("### ğŸ¥§ ç±»åˆ«åˆ†å¸ƒ")
        category_chart = create_category_pie_chart(df)
        if category_chart:
            st.plotly_chart(category_chart, use_container_width=True)
    
    # æ ‡ç­¾äº‘
    st.markdown("### ğŸ·ï¸ æ ‡ç­¾åˆ†æ")
    all_tags = []
    for tags in df['tags'].dropna():
        tag_list = re.split(r'[,ï¼Œ\s]+', str(tags))
        all_tags.extend([tag.strip() for tag in tag_list if tag.strip() and tag.strip().startswith('#')])
    
    if all_tags:
        tag_counts = Counter(all_tags)
        top_tags = dict(tag_counts.most_common(20))
        
        # åˆ›å»ºæ ‡ç­¾äº‘å¯è§†åŒ–
        tag_df = pd.DataFrame(list(top_tags.items()), columns=['æ ‡ç­¾', 'æ¬¡æ•°'])
        fig = px.bar(tag_df, x='æ¬¡æ•°', y='æ ‡ç­¾', orientation='h', 
                     title='å¸¸ç”¨æ ‡ç­¾ç»Ÿè®¡ (Top 20)')
        fig.update_layout(
            font=dict(family=FONTS['family'], size=12),
            plot_bgcolor='white',
            paper_bgcolor='white'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # å¯¼å‡ºç»Ÿè®¡æ•°æ®
    st.markdown("### ğŸ“¤ å¯¼å‡ºæ•°æ®")
    if st.button("ğŸ“Š å¯¼å‡ºCSVæŠ¥å‘Š", use_container_width=True):
        csv_data = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            "ğŸ’¾ ä¸‹è½½CSV",
            csv_data,
            file_name=f"lab_report_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            use_container_width=True
        )

# ==================== å¯¼å‡ºåŠŸèƒ½ï¼ˆç»§ç»­ï¼‰ ====================
def build_archive_markdown(rows):
    """æ„å»ºå½’æ¡£Markdown"""
    sections = [build_record_markdown(r) for r in rows]
    return "\n\n---\n\n".join(sections)

def build_archive_docx_bytes(rows):
    """æ„å»ºå½’æ¡£DOCX"""
    doc = Document()
    for idx, row in enumerate(rows):
        if idx > 0:
            doc.add_page_break()
        row = normalize_task_row(row)
        doc.add_heading(row.get("task_name", "å®éªŒè®°å½•"), level=1)
        doc.add_paragraph(f"æ—¥æœŸï¼š{row.get('date', '-')}")
        doc.add_paragraph(f"ç±»å‹ï¼š{row.get('category', '-')}")
        doc.add_paragraph(f"æ ‡ç­¾ï¼š{row.get('tags') or '-'}")
        doc.add_heading("å®éªŒè®°å½•", level=2)
        doc.add_paragraph(row.get("details") or "(æš‚æ— å®éªŒè®°å½•)")
    bio = BytesIO()
    doc.save(bio)
    return bio.getvalue()

def get_archive_exports(rows):
    """è·å–å½’æ¡£å¯¼å‡ºæ•°æ®"""
    if not rows:
        return []
    rows = [normalize_task_row(r) for r in rows]
    timestamp = datetime.now().strftime("%Y%m%d")
    base = f"lab_archive_{timestamp}"
    md_bytes = build_archive_markdown(rows).encode("utf-8")
    docx_bytes = build_archive_docx_bytes(rows)
    return [
        ("MD", f"{base}.md", md_bytes, "text/markdown"),
        ("DOCX", f"{base}.docx", docx_bytes, "application/vnd.openxmlformats-officedocument.wordprocessingml.document"),
    ]

def build_weekly_report_fallback(records, start_date, end_date):
    """æ— AIæ—¶çš„ç®€å•å‘¨æŠ¥æ‹¼æ¥"""
    lines = [f"# å‘¨æŠ¥ï¼ˆ{start_date} ~ {end_date}ï¼‰", ""]
    for row in records:
        snippet = (row.get("details") or "").replace("\n", " ")
        snippet = re.sub(r"\s+", " ", snippet)
        if len(snippet) > 160:
            snippet = snippet[:160] + "â€¦"
        lines.append(f"- {row.get('date', '')} {row.get('task_name', '')}ï¼š{snippet}")
    return "\n".join(lines)
